<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://chokevin8.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chokevin8.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-19T05:57:12+00:00</updated><id>https://chokevin8.github.io/feed.xml</id><title type="html">Kevin (Won June) Cho</title><subtitle></subtitle><entry><title type="html">(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 5</title><link href="https://chokevin8.github.io/blog/2023/stable-diffusion-part5/" rel="alternate" type="text/html" title="(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 5"/><published>2023-11-18T00:00:00+00:00</published><updated>2023-11-18T00:00:00+00:00</updated><id>https://chokevin8.github.io/blog/2023/stable-diffusion-part5</id><content type="html" xml:base="https://chokevin8.github.io/blog/2023/stable-diffusion-part5/"><![CDATA[<h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <h3 id="latentstable-diffusion-fully-explained-part-1"><a href="/blog/2023/stable-diffusion/">Latent/Stable Diffusion Fully Explained! (Part 1)</a></h3> <ul> <li> <h3 id="introduction">Introduction</h3> </li> <li> <h3 id="stable-diffusion-vs-gan">Stable Diffusion vs GAN</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-2"><a href="/blog/2023/stable-diffusion-part2/">Latent/Stable Diffusion Fully Explained! (Part 2)</a></h3> <ul> <li> <h3 id="motivation">Motivation</h3> </li> <li> <h3 id="model-architecture">Model Architecture</h3> </li> <li> <h3 id="experiments--results">Experiments &amp; Results</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-3"><a href="/blog/2023/stable-diffusion-part3/">Latent/Stable Diffusion Fully Explained! (Part 3)</a></h3> <ul> <li> <h3 id="vaes-and-elbo">VAEs and ELBO</h3> </li> <li> <h3 id="model-objective">Model Objective</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-4-this-blog"><a href="#stable-diffusion-in-numbers-2">Latent/Stable Diffusion Fully Explained! (Part 4)</a> (This Blog!)</h3> <ul> <li> <h3 id="different-view-on-model-objective"><a href="#model-objective2">Different View on Model Objective</a></h3> </li> <li> <h3 id="training-and-inference-ddim-vs-ddpm"><a href="#training-inference">Training and Inference (DDIM vs DDPM)</a></h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-5"><a href="/blog/2023/stable-diffusion-part5/">Latent/Stable Diffusion Fully Explained! (Part 5)</a></h3> <ul> <li> <h3 id="conditioning">Conditioning</h3> </li> <li> <h3 id="classifier-free-guidance">Classifier-Free Guidance</h3> </li> <li> <h3 id="summary">Summary</h3> </li> </ul> <hr/> <p><em>Note: For other parts, please click the link above in the table of contents.</em></p> <p><a id="stable-diffusion-in-numbers-2"></a></p> <h2 id="stable-diffusion-in-numbers-continued"><strong>Stable Diffusion In Numbers Continued</strong></h2> <p><strong><em>Currently work in progress, will be updated soon!</em></strong></p>]]></content><author><name></name></author><category term="posts"/><category term="concept-review"/><category term="generative-model"/><summary type="html"><![CDATA[Explanation of conditioning and classifier/classifier-free guidance.]]></summary></entry><entry><title type="html">(Concept Review/Segmentation Models) How to use Class Activation Maps (CAM) for Explainable AI in Semantic Segmentation</title><link href="https://chokevin8.github.io/blog/2023/class-activation-maps/" rel="alternate" type="text/html" title="(Concept Review/Segmentation Models) How to use Class Activation Maps (CAM) for Explainable AI in Semantic Segmentation"/><published>2023-10-20T00:00:00+00:00</published><updated>2023-10-20T00:00:00+00:00</updated><id>https://chokevin8.github.io/blog/2023/class-activation-maps</id><content type="html" xml:base="https://chokevin8.github.io/blog/2023/class-activation-maps/"><![CDATA[<p>In this post, I will briefly describe Class Activation Maps (CAM) and some of its popular subtypes, usages in semantic segmentation, and then finally post some code and results in utilizing CAM in my own semantic segmentation project using <a href="/projects/1_project/">H&amp;E images</a>.</p> <hr/> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <ul> <li> <h3 id="what-are-class-activation-maps-cam"><a href="#what-are-cam">What are Class Activation Maps (CAM)?</a></h3> </li> <li> <h3 id="cams-in-image-semantic-segmentation"><a href="#cams-in-image-seg">CAMs in Image Semantic Segmentation:</a></h3> </li> <li> <h3 id="utilizing-cam-in-my-project"><a href="#cam-in-my-proj">Utilizing CAM in My Project</a></h3> </li> </ul> <hr/> <p><a id="what-are-cam"></a></p> <h2 id="what-are-class-activation-maps-cam-"><strong>What are Class Activation Maps (CAM) ?</strong></h2> <p><em>Class activation maps</em>, henceforth referred to as <em>CAM</em>, can be thought of as heatmaps that can highlight the regions of a query image that are most important for the network’s classification or segmentation decision. Before introducing CAMs, <em>why do we want to look at CAMs in the first place?</em></p> <p>CAMs are probably one of the most popular and easy-to-do explainable AI metrics, which aim to understand and interpret the decisions made by built machine learning models. Let’s come up with an example where CAMs would be useful: Let’s say I work for a AI company and we built a machine learning model that looks at a chest x-ray and is able to detect a rare disease that occurs 1% of the time with 99% sensitivity (so the model itself is sensitive (or has high recall), so it isn’t saying no all the time and has 99% accuracy). However, by utilizing CAMs before deploying the model, I find out that the model is actually only looking at a circle at the corner of the x-ray image to detect the rare disease. Upon checking the images used for training/validation/testing, they all had a circle at the corner that the hospital used to specify that this was a positive image. <strong>Without CAMs, we would have no idea this was the case and would’ve been deploying a completely useless model!</strong></p> <p>Like the above made-up example, traditionally, CAMs were developed to be used for image classification, as utilization of CAM was limited to specific types of architectures, which are CNNs with global average pooling (GAP) and a final fully connected layer. Most image classification models use GAP and a final fully connected layer followed by the output activation function (assume multi-class, so we use softmax) for turning logits into final prediction probability/results. The last layer before the GAP and fully connected layer is the layer that holds the “feature map”, which captures subtle, fine semantic details of the training images. The implementation of CAM is beautifully summarized in the diagram below:</p> <p><img src="/assets/images/CAMs/CAM_summarized.jpg" width="835" height="392" class="center"/></p> <figcaption> Diagram showing CAM for an image classification task. </figcaption> <p><br/></p> <p>As seen in the above diagram, the class activation map is generated by a linear combination of all the \(n\) weights (weights in the fully connected layer) for the specific class (in the above case, the Australian terrier) and all the \(n\) feature maps. For example, in the diagram above, we can tell that \(w_2\) would have a higher weight than \(w_1\) since the feature map \(F_2\) has to do with the Australian terrier. Mathematically, the CAM can be represented as:</p> <p> $$Y_c = \sum_{k} {w_k}^{c} \cdot \frac{1}{Z} \sum_{i}\sum_{j} A_{(i,j)}^{k}$$ $$Y_c = \sum_{k} {w_k}^{c} \cdot F^{k} \quad (1)$$ </p> <p>,where \(Y_c\) is the activation score (CAM) for \(c\), which is the specific class (like Australian terrier). \(k\) is the number of feature maps and therefore \({w_k}^{c}\) is the weight for \(k\)th feature map for class \(c\). Lastly, \(A_{(i,j)}^{k}\) is the \(k\)th feature map for pixel coordinate \((i,j)\), which is summed over all \(i\) and \(j\) and divided by total number of pixels \(Z\) to return our global average pooled feature map \(F^{k}\). We can see that \(\frac{1}{Z} \sum_{i}\sum_{j}\) is the mathematical representation of global average pooling (GAP). Finally, since feature maps are downsampled compared to the original image size, we need to perform bi-linear interpolation on the CAM to upsample for us to visualize the overlay of the CAM on the query image like shown in the diagram above.</p> <p>However, as mentioned above, the general CAM method requires an architecture that includes all of the three:</p> <ol> <li>A feature map, or the penultimate layer of the model.</li> <li>A global average pooling operation to the feature map.</li> <li>A final fully connected layer followed by an activation function to produce prediction labels.</li> </ol> <p>Since not all methods satisfy all three requirements above, CAM was only limited to certain types of architecture, making it unavailable to use for other types of CNN architectures that are dense like VGG, handle multi-modal inputs/perform reinforcement learning.</p> <p><a id="cams-in-image-seg"></a></p> <h2 id="cams-in-image-semantic-segmentation-1"><strong>CAMs in Image Semantic Segmentation:</strong></h2> <p>As mentioned previously, the three requirements for CAM severely limited the usage to certain types of architectures, and was totally not applicable for non-CNN based models such as vision transformers, or ViTs (to be fair, when CAM was first released, transformers were not a thing yet). Therefore, GradCAM, or Gradient-weighted CAMs, were widely used which essentially replaces the weights of the fully connected layer with calculated gradients that flow back into the last convolutional layer. To show that this is true, consider taking the gradient of the activation score for class \(c\) (\(Y^{c}\)) with respect to feature map \(F^{k}\) from the above CAM equation, or equation #1:</p> <p> $$ Y_c = \sum_{k} {w_k}^{c} \cdot F^{k}$$ $$\frac{\delta Y^{c}}{\delta F^{k}} = \frac{\frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}} {\frac{\delta F^{k}}{\delta A_{(i,j)}^{k}}} $$ $$\text{Recall from above: } F^{k} = \frac{1}{Z} \sum_{i}\sum_{j} \text{ so: } \frac{\delta F^{k}}{\delta A_{(i,j)}^{k}} = \frac{1}{Z}$$ $$\text{Then: } \frac{\delta Y^{c}}{\delta F^{k}} = \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \cdot Z $$ $$\text{Recall from above: } Y_c = \sum_{k} {w_k}^{c} \cdot F^{k} \text{ so: } \frac{\delta Y^{c}}{\delta F^{k}} = {w_k}^{c}$$ $$\text{Then: } {w_k}^{c} = Z \cdot \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (2)$$ </p> <p>Now note that above equation #2 is only for pixel location \((i,j)\), so let’s sum over all pixels. Note that \(\sum_{i}\sum_{j} 1 = Z\):</p> <p> $$\sum_{i}\sum_{j} {w_k}^{c} = Z \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}$$ $$Z{w_k}^{c} = Z \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}$$ $${w_k}^{c} = \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (3)$$ </p> <p>The equation #3 is an important result, this shows us that the weights for feature map \(k\) for class \(c\) are equal to the gradient of the activation score with respect to the \(k\)th feature map. But remember, we’re no longer using the weights of the fully connected layer! By re-introducing the normalization constant \(1/Z\) for global average pooling, and pooling over the gradients instead, we obtain neural importance weights \(\alpha_k^{c}\) instead:</p> <p> $${\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (4)$$ </p> <p>We can think of this neural importance weight \({\alpha_k}^{c}\) as the equivalent to the \({w_k}^{c}\) in vanilla CAMs. Therefore, use equation #1 for CAM above and sum over all \(k\) feature maps to find our equation for evaluating our activation score for Grad-CAM:</p> <p> $$Y_c = \sum_{k} {\alpha_k}^{c} \cdot F^{k} \text{ where } {\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} (5)$$ </p> <p>Equation #5 is one-step short of completion. Why? The gradients can be negative or positive! We are only interested in finding features that have a positive influence on class \(c\). This means that the activation score should be positive, and to have a positive gradient, the feature map should be positive as well. Therefore, with Grad-CAM we can highlight regions in the image, or features, that contribute to increasing the class activation score. On the other hand, negative gradients most likely belong to features in the image that are correlated with classes that are not \(c\), and therefore we apply ReLU function to suppress negative gradients and only keep positive gradients:</p> <p> $$Y_c = ReLU (\sum_{k} {\alpha_k}^{c} \cdot F^{k}) \text{ where } {\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} (6)$$ </p> <p>Equation #6 above is the final equation for Grad-CAM. Now, even though CAMs were widely used for classification, they could definitely also be used for semantic segmentation tasks where each pixel is labeled as a class. In this case, however, we have to modify equation #6 a bit. This is because image classification outputs a single class distribution (ex. this image is a dog), image semantic segmentation doesn’t, as it outputs logits for every pixel \((a,b)\) predicted for class \(c\). Therefore, it makes sense to sum all of these pixels as the activation score so that it becomes a single class distribution like image classification. We therefore modify the \(Y^{c}\) in the gradient to \(\sum_{(a,b) \in M}{Y_{(a,b)}}^{c}\) where \(M\) is a set of all pixel indices that belong to class \(c\) in the segmentation prediction. The final equation for Grad-CAM in image segmentation is shown below:</p> <p> $$Y_c = ReLU (\sum_{k} {\alpha_k}^{c} \cdot F^{k}) \text{ where } {\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta \sum_{(a,b) \in M}{Y_{(a,b)}}^{c}}{\delta A_{(i,j)}^{k}} (7)$$ </p> <p>However, if we look at the equation #6 or #7 above for Grad-CAM carefully, there is a critical issue- when calculating the neural importance weight \({\alpha_k}^{c}\) the gradients are averaged due to global average pooling (GAP). Why can this be a problem? Look at the diagram below: <img src="/assets/images/hirescam.png" width="744" height="480" class="center"/></p> <figcaption> Diagram comparing HiResCAM and GradCAM and how HiResCAM alleviates the problem of averaging the gradients in GradCAM. </figcaption> <p><br/> As seen in the diagram above, we see an example of a 3 x 3 feature map. Note the positive/negative pattern of this feature map. With HiResCAM, the feature map is multiplied by the gradients in an element-wise matter, and we can see that the positive and negative gradients are taken into account in the resulting HiResCAM. However, with Grad-CAM the gradients are all averaged out, and therefore the negative gradients are actually suppressed, and therefore the resulting Grad-CAM retains its original positive/negative feature map pattern. Therefore, we see that HiResCAM produces accurate attention.</p> <p><a id="cam-in-my-proj"></a></p> <h2 id="utilizing-cam-in-my-project-1"><strong>Utilizing CAM in My Project:</strong></h2> <p>Now that we know the differences between GradCAM and HiResCAM, below is the code that I utilized to generate CAM for my skin H&amp;E tissue images to assess how my DeepLabv3+ image segmentation segments the classes and if it is cheating or not!</p> <p>First import relevant packages, including our pytorch-grad-cam library from the official <a href="https://github.com/jacobgil/pytorch-grad-cam">repo</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">natsort</span> <span class="kn">import</span> <span class="n">natsorted</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam.utils.image</span> <span class="kn">import</span> <span class="n">show_cam_on_image</span><span class="p">,</span> <span class="n">preprocess_image</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam</span> <span class="kn">import</span> <span class="n">GradCAM</span><span class="p">,</span> <span class="n">HiResCAM</span><span class="p">,</span> <span class="n">GradCAMElementWise</span><span class="p">,</span> <span class="n">ScoreCAM</span><span class="p">,</span> <span class="n">GradCAMPlusPlus</span><span class="p">,</span> <span class="n">AblationCAM</span><span class="p">,</span> <span class="n">XGradCAM</span><span class="p">,</span> <span class="n">EigenCAM</span><span class="p">,</span> <span class="n">EigenGradCAM</span><span class="p">,</span> <span class="n">FullGrad</span><span class="p">,</span> <span class="n">LayerCAM</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam.utils.model_targets</span> <span class="kn">import</span> <span class="n">ClassifierOutputTarget</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam.utils.image</span> <span class="kn">import</span> <span class="n">show_cam_on_image</span>
<span class="n">Image</span><span class="p">.</span><span class="n">MAX_IMAGE_PIXELS</span> <span class="o">=</span> <span class="bp">None</span>
<span class="kn">import</span> <span class="n">segmentation_models_pytorch</span> <span class="k">as</span> <span class="n">smp</span>
<span class="kn">import</span> <span class="n">albumentations</span> <span class="k">as</span> <span class="n">A</span>
<span class="kn">from</span> <span class="n">albumentations.pytorch</span> <span class="kn">import</span> <span class="n">ToTensorV2</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Then, write your own function to load relevant model (for me, my DeepLabV3+ segmentation model) and if necessary, your own dataloading function as well:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_transforms</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="nc">ToTensorV2</span><span class="p">()])</span>  <span class="c1">#just convert to tensor
</span>
<span class="k">class</span> <span class="nc">TestDataSet</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="c1"># initialize imagepath, transforms:
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_paths</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transforms</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>

    <span class="c1"># define main function to read image, apply transform function and return the transformed images.
</span>    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># albumentations
</span>            <span class="n">transformed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transforms</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">image</span> <span class="c1"># return tensors of equal dtype and size
</span>        <span class="c1"># image is size 3x1024x1024 and mask and bin_mask is size 1x1024x1024 (need dummy dimension to match dimension)
</span>
<span class="c1"># define dataloading function to use above dataset to return train and val dataloaders:
</span><span class="k">def</span> <span class="nf">load_test_dataset</span><span class="p">():</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">TestDataSet</span><span class="p">(</span><span class="n">image_paths</span><span class="o">=</span><span class="n">test_images</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="n">test_transforms</span><span class="p">)</span>
    <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_dataloader</span>  <span class="c1"># return train and val dataloaders
</span></code></pre></div></div> <p>Then write a function to return the predicted segmentation tissue map (for all classes) for the image to generate HiResCAM for:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_path</span> <span class="o">=</span> <span class="c1"># your own path to folder containing images to test for CAM
</span><span class="n">test_images</span> <span class="o">=</span> <span class="c1"># complete path of the single image to test for CAM within image_path
</span><span class="n">test_dataloader</span> <span class="o">=</span> <span class="nf">load_test_dataset</span><span class="p">()</span> 
<span class="nd">@torch.no_grad</span><span class="p">()</span>  <span class="c1">#decorator to disable gradient calc
</span><span class="k">def</span> <span class="nf">return_image_mask</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">weight_dir</span> <span class="o">=</span> <span class="c1"># your own path to the saved model weights
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">weight_dir</span><span class="p">))</span>  <span class="c1">#load model weights
</span>    <span class="n">pbar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">'</span><span class="s">Inference</span><span class="sh">'</span><span class="p">,</span> <span class="n">colour</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">image_path</span><span class="p">)</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>  <span class="c1"># eval stage
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>  <span class="c1">#move tensor to gpu
</span>        <span class="n">prediction</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">()</span> <span class="c1">#softmax for multiclass
</span>    <span class="k">return</span> <span class="n">prediction</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="nf">return_image_mask</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">test_dataloader</span><span class="p">,</span><span class="n">device</span><span class="p">)</span> <span class="c1">#predicted segmentation tissue map
</span></code></pre></div></div> <p>Then, preprocess the image (imagenet mean/std) to generate HiResCAM for:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rgb_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">))</span>
<span class="n">rgb_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span> <span class="c1">#imagenet mean/std
</span></code></pre></div></div> <p>Then, define a class to return the sum of the predictions of a specific chosen class (ex. class ECM for skin tissue), or the “target”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SemanticSegmentationTarget</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">category</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">category</span> <span class="o">=</span> <span class="n">category</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mask</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model_output</span><span class="p">):</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">model_output</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">category</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:</span> <span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">mask</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
</code></pre></div></div> <p>Then, below is the most important part: For a specific chosen class (ex. class ECM for skin tissue), return the “target”, choose a layer of interest from the loaded model, choose a CAM method (we choose HiResCAM, which was explained above) and utilize pytorch-grad-cam’s functions to generate our CAM image!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">he_mask</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:].</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># for skin: {"corneum" : 1,"spinosum": 2,"hairshaft":3,"hairfollicle":4,"smoothmuscle":5,"oil":6,"sweat":7,"nerve":8,"bloodvessel":9,"ecm":10,"fat":11,"white/background":12}
</span><span class="n">class_category</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># 6 = oil glands
</span><span class="n">he_mask_float</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">he_mask</span> <span class="o">==</span> <span class="n">class_category</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="nc">SemanticSegmentationTarget</span><span class="p">(</span><span class="n">class_category</span><span class="p">,</span> <span class="n">he_mask_float</span><span class="p">)]</span> <span class="c1"># return targets
</span><span class="n">target_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer4</span><span class="p">]</span> <span class="c1"># we choose last layer
</span><span class="k">with</span> <span class="nc">HiResCAM</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_layers</span> <span class="o">=</span> <span class="n">target_layers</span><span class="p">,</span> <span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span> <span class="k">as</span> <span class="n">cam</span><span class="p">:</span>
    <span class="n">grayscale_cam</span> <span class="o">=</span> <span class="nf">cam</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">,</span><span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)[</span><span class="mi">0</span><span class="p">,:]</span> <span class="c1"># return CAM
</span>    <span class="n">cam_image</span> <span class="o">=</span> <span class="nf">show_cam_on_image</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">,</span> <span class="n">grayscale_cam</span><span class="p">,</span> <span class="n">use_rgb</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># overlay CAM with original rgb image
</span><span class="n">Image</span><span class="p">.</span><span class="nf">fromarray</span><span class="p">(</span><span class="n">cam_image</span><span class="p">)</span> <span class="c1">#visualize resulting CAM
</span></code></pre></div></div> <p>Finally, let’s look at some examples of CAMs for some of my skin H&amp;E tissue images, and hopefully we’ll see that my trained model is actually focusing on the right parts of the images and not cheating!</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/white_background_GradCAMElementWise-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/white_background_GradCAMElementWise-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/white_background_GradCAMElementWise-1400.webp"/> <img src="/assets/images/CAMs/white_background_GradCAMElementWise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="White/background GradCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/white_background_HiResCAM-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/white_background_HiResCAM-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/white_background_HiResCAM-1400.webp"/> <img src="/assets/images/CAMs/white_background_HiResCAM.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="White/background HiResCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/bloodvessel_GradCAMElementWise-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/bloodvessel_GradCAMElementWise-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/bloodvessel_GradCAMElementWise-1400.webp"/> <img src="/assets/images/CAMs/bloodvessel_GradCAMElementWise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Blood vessel GradCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/bloodvessel_HiResCAM-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/bloodvessel_HiResCAM-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/bloodvessel_HiResCAM-1400.webp"/> <img src="/assets/images/CAMs/bloodvessel_HiResCAM.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Blood vessel HiResCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/oil_GradCAMElementWise-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/oil_GradCAMElementWise-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/oil_GradCAMElementWise-1400.webp"/> <img src="/assets/images/CAMs/oil_GradCAMElementWise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Oil GradCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/oil_HiResCAM-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/oil_HiResCAM-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/oil_HiResCAM-1400.webp"/> <img src="/assets/images/CAMs/oil_HiResCAM.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Oil HiResCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <p>The left images are GradCAMs, and the right images are HiResCAMs Now let’s analyze the images of each row. The first row are the CAMs for the background, and we can see that there isn’t a big difference between the two, except that HiResCAM does show a more “accurate” depiction, as it is a faithful explanation after all. The second row are the CAMs for the blood vessels, and we can see that HiResCAM also has a more “accurate” depiction while GradCAM shows activation scores for spots that are not blood vessel-specific. Lastly, the third row are the CAMs for the oil glands, and this is where GradCAM is a bit misleading. GradCAM does highlight the oil glands successfully, but when looking at HiResCAM, we can see that the model doesn’t only look at oil glands. Therefore, with HiResCAM, I can see that the model is also mostly looking at nearby ECM and hair follicle areas for segmenting oil glands, which is quite interesting.</p> <p>The last example like above is the reason why we must continue to explore and try different types of CAMs, and also explore other options of explainable AI as well. Hope this helps!</p> <hr/> <p><em>Image credits to:</em></p> <ul> <li><a href="http://cnnlocalization.csail.mit.edu/">Image Classification CAM Diagram</a></li> <li><a href="https://arxiv.org/pdf/2011.08891.pdf">HiResCAM Diagram</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="concept-review"/><category term="segmentation-model"/><summary type="html"><![CDATA[Introduction to CAM, explanation of GradCAM and HiResCAM in semantic segmentation, showcasing some example code for CAM and CAM image results in my project!]]></summary></entry><entry><title type="html">(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 4</title><link href="https://chokevin8.github.io/blog/2023/stable-diffusion-part4/" rel="alternate" type="text/html" title="(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 4"/><published>2023-09-10T00:00:00+00:00</published><updated>2023-09-10T00:00:00+00:00</updated><id>https://chokevin8.github.io/blog/2023/stable-diffusion-part4</id><content type="html" xml:base="https://chokevin8.github.io/blog/2023/stable-diffusion-part4/"><![CDATA[<hr/> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <h3 id="latentstable-diffusion-fully-explained-part-1"><a href="/blog/2023/stable-diffusion/">Latent/Stable Diffusion Fully Explained! (Part 1)</a></h3> <ul> <li> <h3 id="introduction">Introduction</h3> </li> <li> <h3 id="stable-diffusion-vs-gan">Stable Diffusion vs GAN</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-2"><a href="/blog/2023/stable-diffusion-part2/">Latent/Stable Diffusion Fully Explained! (Part 2)</a></h3> <ul> <li> <h3 id="motivation">Motivation</h3> </li> <li> <h3 id="model-architecture">Model Architecture</h3> </li> <li> <h3 id="experiments--results">Experiments &amp; Results</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-3"><a href="/blog/2023/stable-diffusion-part3/">Latent/Stable Diffusion Fully Explained! (Part 3)</a></h3> <ul> <li> <h3 id="vaes-and-elbo">VAEs and ELBO</h3> </li> <li> <h3 id="model-objective">Model Objective</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-4-this-blog"><a href="#stable-diffusion-in-numbers-2">Latent/Stable Diffusion Fully Explained! (Part 4)</a> (This Blog!)</h3> <ul> <li> <h3 id="different-view-on-model-objective"><a href="#model-objective2">Different View on Model Objective</a></h3> </li> <li> <h3 id="training-and-inference-ddim-vs-ddpm"><a href="#training-inference">Training and Inference (DDIM vs DDPM)</a></h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-5--coming-soon"><a href="/blog/2023/stable-diffusion-part5/">Latent/Stable Diffusion Fully Explained! (Part 5- Coming Soon!)</a></h3> <ul> <li> <h3 id="conditioning">Conditioning</h3> </li> <li> <h3 id="classifier-free-guidance">Classifier-Free Guidance</h3> </li> <li> <h3 id="summary">Summary</h3> </li> </ul> <hr/> <p><em>Note: For other parts, please click the link above in the table of contents.</em></p> <p><a id="stable-diffusion-in-numbers-2"></a></p> <h2 id="stable-diffusion-in-numbers-continued"><strong>Stable Diffusion In Numbers Continued</strong></h2> <p><a id="#model-objective2"></a></p> <h3 id="different-view-on-model-objective-1"><strong>Different View on Model Objective</strong></h3> <p>In this last part of the blog, I want to cover the mathematical details of conditioning and also classifier-free guidance. Before, that let’s briefly look at the algorithms for training and inference, and view the training objective we derived in a different way.</p> <p>Recall equation #17 from the previous part of the blog, or the final training objective of our LDM:</p> <p> $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} \frac{\hat{\alpha}_{t-1}(1-\alpha_t)^{2}}{(1-\hat{\alpha_t)^{2}}} [{||(\hat{x}_{\theta}(x_t,t)-x_0)||}^{2}] $$ </p> <p>Comparing this to equation #1 in the paper which describes the training objective, we can see that it is a bit different, as our equation above has some extra terms and is a MSE between predicted and ground truth original image, not noise as seen below.</p> <p><em>Equation #1 (Training objective) in the paper:</em></p> <p> $$L_{LDM} = ||\epsilon - \epsilon_{\theta}(x_t,t)||^2$$ </p> <p>So how are these <em>two somehow equivalent</em>? Well, let’s interpret our training objective in a different way and we’ll see how these two are connected. Recall equation #5 from the last blog (noted as equation #1 below), or the reparametrization trick we used for forward diffusion \(q(x_t \mid x_0)\) to calculate \(x_t\) in terms of the \(\hat{\alpha}\)s. Let’s rearrange this equation in terms of \(x_0\) instead!</p> <p> $$ x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}{\epsilon}_0 \quad (1)$$ $$ x_0 = \frac{x_t - \sqrt{1-\hat{\alpha}_t}{\epsilon}_0}{\sqrt{\hat{\alpha}_t}} \quad (1)$$ </p> <p>Then, recall the equation (equation #2 below) we derived for the mean of the ground truth denoising transition step distribution \(q(x_{t-1} \mid x_t,x_0)\), as we calculated this to minimize the KL-divergence of the ground truth and desired approximate transition step distribution to derive our training objective:</p> \[\mu_q = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}} \quad (2)\] <p>Now, plug equation #1 in to \(x_0\) of equation #2 above:</p> <p> $$\mu_q = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\frac{x_t - \sqrt{1-\hat{\alpha}_t}{\epsilon}_0}{\sqrt{\hat{\alpha}_t}}}{1-\hat{\alpha_t}} \quad (2)$$ </p> <p>Skipping the rearranging algebra (you can try this yourself if you want to), we end up with:</p> <p> $$\mu_q = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{\sqrt{1-\alpha_t}}{\sqrt{1-\hat{\alpha_t}}\sqrt{\alpha_t}}{\epsilon}_0 \quad (3)$$ </p> <p>Then, like before, to find the mean of the desired approximate denoising transition distribution \(\mu_{\theta}\), we simply replace the ground truth noise \({\epsilon}_0\) (since we don’t know ground truth distribution!) with a neural network that parametrizes \(\hat{\epsilon}_{\theta}(x_t,t)\) to predict \(\epsilon_0\) as accurately as possible to make our approximate denoising step as similar to the ground truth denoising step as possible:</p> <p> $$\mu_{\theta} = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{\sqrt{1-\alpha_t}}{\sqrt{1-\hat{\alpha_t}}\sqrt{\alpha_t}}\hat{\epsilon}_{\theta}(x_t,t) \quad (4)$$ </p> <p>Now that equations #3 and #4 above both tell us the mean of both distributions, like what we did before, we find the KL divergence between the two. Recall the equation for calculating the KL-divergence between two Gaussians, and plug in to find the “new” training objective:</p> <p> $$ D_{KL}(\mathcal{N}(x;\mu_x,\Sigma_x) || \mathcal{N}(y;\mu_y,\Sigma_y)) = \frac{1}{2} [ \log \frac{\Sigma_y}{\Sigma_x} - d + tr({\Sigma_y}^{-1}\Sigma_x) + (\mu_y - \mu_x) ^ {T} {\Sigma_y}^{-1} (\mu_y - \mu_x) ] $$ </p> <p>Skipping the rearranging algebra again, we end up with our “new”, different interpretation of our training objective:</p> <p> $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} \frac{(1-\alpha_t)^{2}}{(1-\hat{\alpha_t})\alpha_t}[{||\epsilon_0 - \hat{\epsilon}_{\theta}(x_t,t)||}^{2}] \quad (5)$$ </p> <p>Equation #5 above is our “new” training objective, instead of predicting ground truth image, we predict the ground truth noise instead here. Empirically, depending on the use case, it may work better to predict the noise instead of the image and vice versa, and it seems like the authors of the paper decided to predict the noise. Note that the objective function finalizes to equation #6 below because it was empirically proven that getting rid of the coefficient in front of the MSE term actually performed better when evaluating the performance of diffusion models. This final objective function below (equation #6) is equivalent to the loss function the authors use, which we already saw above:</p> <p> $$L_{LDM} = ||\epsilon_0 - \epsilon_{\theta}(x_t,t)||^2 \quad (6)$$ </p> <p>Therefore, we simply end up with the mean squared error (MSE) between the ground truth noise \(\epsilon_0\) and the predicted noise \(\epsilon_{\theta}(x_t,t)\). Simply put, the decoder \(\hat{\epsilon}_{\theta}(x_t,t)\) (which is the U-Net) learns to predict the ground truth source noise \(\epsilon_0\) that is randomly sampled from \(\mathcal{N}(0, 1)\). The predicted source noise is the noise that originally brought the original image \(x_0\) to the pure noised (image) \(x_t\) via forward diffusion. As stated in the paper, this can also be seen as a sequence of \(T\) equally weighted autoencoders from \(t = 1,2....t-1,T\) which predicts a denoised variant of their input \(x_t\) in a Markovian fashion. As timestep reaches T, this Markovian process will then slowly converge to the ground truth input image \(x_0\), assuming the training of the decoder went well.</p> <p><a id="training-inference"></a></p> <h3 id="training-and-inference-ddim-vs-ddpm-1"><strong><em>Training and Inference (DDIM vs DDPM):</em></strong></h3> <p>Now that we’ve derived the training (loss) objective from scratch, let’s briefly go over the entire training and the inference algorithm:</p> <p><img src="/assets/images/train_inference_algorithm.png" width="985" height="250" class="center"/></p> <figcaption>The training and inference algorithm, summarized.</figcaption> <p><br/> Let’s first look at the training algorithm:</p> <ol> <li>We repeat the below process (steps 2~5) until convergence or a preset number of epochs.</li> <li>Sample an image \(x_0\) from our dataset/data distribution, \(q(x_0)\).</li> <li>Sample t, or timestep from 1 to \(T\).</li> <li>Sample noise from a normal distribution \(\epsilon \sim \mathcal{N}(0, I)\)</li> <li>Take gradient descent step on the training objective we just derived \(L_{LDM} = ||\epsilon - \epsilon_{\theta}(x_t,t)||^2\) with respect to \(\theta\), which is the parameters of the weights and biases of the decoder.</li> </ol> <p>Not too bad! What about the above sampling algorithm?</p> <p>Before describing the sampling process, let’s look back at the training objective, specifically regarding the value of T, or the timesteps for the forward process. Above, we described this as a Markovian process, and ideally we would like to <em>maximize</em> \(T\) or the number of timesteps in the forward diffusion so that the reverse process can be as close to a Gaussian as possible so that the generative process is accurate and generates a good image quality.</p> <p>However, in this Markovian sampling process called DDPM (short for Denoising Diffusion Probabilistic Models), the \(T\) timesteps have to be performed sequentially, meaning sampling speed is extremely slow, especially compared to fast sampling speeds of predecessors such as GANs. The limitation here is that the forward \(T\) steps and the reverse sampling \(T\) steps must be equal, as we are reversing the forward process for sampling. The above Markovian sampling algorithm is the DDPM sampling algorithm, which will be explained first, but then we will also mention a new, non-Markovian sampling process called DDIM (short for Denoising Diffusion Implicit Models) that is able to accelerate sampling speeds. <strong>Note that the authors of the LDM paper utilized DDIM because of this exact reason.</strong></p> <p>Remember that for sampling, we only need the trained decoder from above (no encoder). Therefore, we sample latent noise \(x_T\) from prior \(p(x_T)\), which is \(\epsilon \sim \mathcal{N}(0, I)\) and then run the series of \(T\) equally weighted autoencoders as mentioned before in a Markovian style. The equation shown in the sampling algorithm is essentially identical to equation #4 above, or:</p> <p> $$ \mu_{\theta} = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{\sqrt{1-\alpha_t}}{\sqrt{1-\hat{\alpha_t}}\sqrt{\alpha_t}}\hat{\epsilon}_{\theta}(x_t,t) $$ </p> <p>The equation in the sampling algorithm at step #4 above just has an additional noise term \(\sigma_tz\) since the reparametrization trick \(x = \mu + \sigma_tz\). Note that in sampling, assuming our training went well, we have our trained neural network \(\hat{\epsilon}_{\theta}(x_t,t)\) that predicts the noise \(\epsilon\) for given input image \(x_t\) (remember that this neural network in our LDM is our U-Net architecture with the (cross) attention layers that predicts the noise given our input image). Then, inputting a timestep \(t\) and original image \(x_t\) to the trained neural network gives us the predicted noise \(\epsilon\), and using that we can sample \(x_{t-1}\) until \(t=1\). When \(t=1\), we have our sampled output of \(x_0\).</p> <p>Let’s summarize this sampling algorithm:</p> <ol> <li>First sample Gaussian noise \(x_T\) from normal distribution.</li> <li>Then, reverse the timesteps for the forward diffusion, and for each timestep, sample noise \(z\) from another independent normal distribution. Note that when \(t=1\), we don’t want to further add noise. Lastly, for each timestep, sample \(x_{t-1}\) according to above equation.</li> <li>Repeat step #2 for each timestep until \(t=1\), and the generated sample is the desired \(x_0\).</li> </ol> <p>However, as discussed above, Denoising Diffusion Implicit Model (DDIM) uses a non-Markovian sampling process that makes the process much quicker. DDPMs use a sampling process that is essentially the reverse of the forward diffusion (\(T\) forward and backward timesteps), while DDIM uses \(S\) steps instead of \(T\) where \(S&lt;T\), by using the fact that the forward diffusion process can be made non-Markovian and therefore the reverse sampling process can also be made non-Markovian. Therefore, the authors of the LDM paper use <em>DDIM over DDPM.</em></p> <p>Recall the forward diffusion process mentioned in the previous part of the blog: \(x_t = \mathcal{N}(x_t; \mu_t = \sqrt{1-\beta_t}x_{t-1},\Sigma_t = \beta_tI)\). This is essentially the forward process of DDPMs, which we can see that the process is Markovian, as \(x_t\) only depends on \(x_{t-1}\). However, in the previous part of the blog, we’ve already shown the forward process that can be made non-Markovian when we were deriving the training objective. Recall the Baye’s rule we used to derive the mean and variance of the approximate denoising step: \(q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\). Note by rearranging the above equation for the forward diffusion step \(q(x_t|x_{t-1},x_0)\) we see that the forward step is no longer Markovian, and this is the forward step for DDIM: \(q(x_t|x_{t-1},x_0) = \frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}\)</p> <p>With this non-Markovian forward diffusion step, the DDIM sampling process is also no longer forced to have the same number of timesteps \(T\). But how do we derive the DDIM sampling process? To derive the DDIM sampling process, we utilize the <em>reparametrization trick</em> again, which we applied previously for forward diffusion. We can use \(x = \mu + \sigma * \epsilon\) to essentially alter our sampling process \(q(x_{t-1}|x_t,x_0)\) to be parametrized by another random variable, a desired standard deviation \(\sigma_t\) (square this for variance). The reparametrization is shown below:</p> <p> $$\text{Recall equation #1:} \text{ } x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}{\epsilon}_0 $$ $$\text{The equation for} \text{ } x_{t-1} \text{ } \text{instead is:} \text{ } x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}x_0 + \sqrt{1-\hat{\alpha}_{t-1}}{\epsilon}_{t-1} $$ $$\text{Add extra term} \text{ } \sigma_t \epsilon \text{ } \text{for reparametrization trick, where} \text{ } \sigma_t^{2} \text{ } \text{is the variance of our distribution.}$$ $$ x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}x_0 + \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\epsilon_t + \sigma_t \epsilon $$ $$\text{Since} \text{ } \epsilon_t = \frac{x_t - \sqrt{\hat{\alpha_t}}x_0}{\sqrt{1-\hat{\alpha_t}}}: \text{ } x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}x_0 + \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\frac{x_t - \sqrt{\hat{\alpha_t}}x_0}{\sqrt{1-\hat{\alpha_t}}} + \sigma_t \epsilon $$ $$\text{Therefore,} \text{ } q(x_{t-1}|x_t,x_0) = \mathcal{N}(x_{t-1};\mu_{t-1} = \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\frac{x_t - \sqrt{\hat{\alpha_t}}x_0}{\sqrt{1-\hat{\alpha_t}}},\Sigma_{t-1}= \sigma_t^{2}I)) $$ $$\text{Recall equation #15 from last blog for variance formulation:} $$ $$q(x_{t-1} \mid x_t,x_0) \sim \mathcal{N}(x_{t-1}; \mu_t = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}},\Sigma_t = \frac{(1-\alpha_t)(1-\hat{\alpha}_{t-1})}{(1-\hat{\alpha_t})}I)$$ $$\text{Then, also recall that} \text{ } \beta_t = 1-\alpha_t \text{ ,} \text{ } \text{therefore our new variance:}$$ $$\hat{\beta_t} = \sigma_t^{2} = \frac{1-\hat{\alpha}_{t-1}}{1-\hat{\alpha_t}} * \beta_t $$ </p> <p>Above shows the mean and the variance of the DDIM denoising step. With above result, we can now let \(\eta = \frac{1-\hat{\alpha}_{t-1}}{1-\hat{\alpha_t}}\) and now \(\sigma_t^{2} = \eta * \hat{\beta_t}\) where \(\eta\) can now be used to control the stochasticity/determinism of the sampling process. As one can see, if \(\eta = 0\), this means that the variance of the above denoising step becomes zero and therefore the sampling becomes deterministic. This means that given an input image, no matter how many different times you sampled, you would end up with similar images with the same high-level features! Therefore, this is why this process is called “denoising diffusion implicit model”, as like other implicit models like GANs, the sampling process is deterministic.</p> <p><strong>But our main point was that DDIM dramatically speeds up the sampling process. We’ve shown that the forward process is non-Markovian, but how do we show that the reverse process requires fewer steps as well?</strong> Recall the derived sampling process from above:</p> <p> $$x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}x_0 + \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\frac{x_t - \sqrt{\hat{\alpha_t}}x_0}{\sqrt{1-\hat{\alpha_t}}} + \sigma_t \epsilon $$ </p> <p>We will now aim to parametrize this sampling process for \(x_{t-1}\) in terms of our trained model. Here, we can see that given \(x_t\), we first make a prediction of \(x_0\), and then use both to make the prediction for \(x_{t-1}\). Now, recall the previously mentioned forward process \(q(x_t|x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha}_t}x_0,\Sigma_t = (1-\hat{\alpha}_t)I)\). With reparametrization trick, we already showed that this gives our forward process to find \(x_t\) given \(x_0\) and our Gaussian noise \(\epsilon\): \(x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}{\epsilon}\). Rearranging the equation for \(x_0\) gives: \(x_0 = \frac{x_t - \sqrt{1-\hat{\alpha}_t}{\epsilon}}{\sqrt{\hat{\alpha}_t}}\). Now, here’s the important part, where do we utilize our trained model if this is the generative process? Recall from above again that <strong>our trained model \(\hat{\epsilon}_{\theta}(x_t,t)\) predicts noise \(\epsilon\) given \(x_t\) and timestep \(t\).</strong> Therefore, we rewrite the equation for \(x_0\) as:</p> <p> $$x_0 = \frac{x_t - \sqrt{1-\hat{\alpha}_t}\hat{\epsilon}_{\theta}(x_t,t)}{\sqrt{\hat{\alpha}_t}}$$ </p> <p>Therefore, when we plug in this equation for \(x_0\) in original sampling process equation, we get:</p> <p> $$x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}\frac{x_t - \sqrt{1-\hat{\alpha}_t}\hat{\epsilon}_{\theta}(x_t,t)}{\sqrt{\hat{\alpha}_t}} + \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\frac{x_t - \sqrt{\hat{\alpha_t}}x_0}{\sqrt{1-\hat{\alpha_t}}} + \sigma_t \epsilon $$. </p> <p>But recall that when we rearrange the same equation \(x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}{\epsilon}\) in terms of \(\epsilon\), we get \(\epsilon = \frac{x_t - \sqrt{\hat{\alpha_t}}x_0}{\sqrt{1-\hat{\alpha_t}}}\). Therefore, we replace that term above with our trained model as well, giving us the final equation for \(x_{t-1}\) for DDIM sampling:</p> <p> $$x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}\frac{x_t - \sqrt{1-\hat{\alpha}_t}\hat{\epsilon}_{\theta}(x_t,t)}{\sqrt{\hat{\alpha}_t}} + \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\hat{\epsilon}_{\theta}(x_t,t) + \sigma_t \epsilon_t $$. $$\text{where: }q(x_{t-1} \mid x_t,x_0) \sim \mathcal{N}(x_{t-1}; \mu_t = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}},\Sigma_t = \frac{(1-\alpha_t)(1-\hat{\alpha}_{t-1})}{(1-\hat{\alpha_t})}I)$$ </p> <p>The above equation is the final equation that allows us to sample \(x_{t-1}\) from given \(x_t\) using our trained model. Suppose the forward process is non-Markovian now, and instead of having all of the Markovian steps from \(x_{1:T}\), we have a subset of \(S\) timesteps \({x_{\tau 1},....x_{\tau S}}\) where \(\tau\) is simply increasing sequence of \(S\) timesteps. Look at the figure below:</p> <p><img src="/assets/images/ddim-sampling.png" width="731" height="227" class="center"/></p> <figcaption>Diagram showing DDIM forward and sampling process in comparison to DDIM.</figcaption> <p><br/></p> <p>The diagram above is a simplified one in that \(\tau = [1,3]\), and the forward DDIM process \(q(x_3 \mid x_1,x_0)\) can be simply reversed by sampling using the above derived sampling process. <strong>Therefore, we see that DDIM utilizes a non-Markovian forward process that uses less timesteps which in turn allows it to use less computations in the reverse step as well.</strong></p> <p> $$x_{t-1} = \sqrt{\hat{\alpha}_{t-1}}\frac{x_t - \sqrt{1-\hat{\alpha}_t}\hat{\epsilon}_{\theta}(x_t,t)}{\sqrt{\hat{\alpha}_t}} + \sqrt{1-\hat{\alpha}_{t-1}-\sigma_t^{2}}\hat{\epsilon}_{\theta}(x_t,t) + \sigma_t \epsilon_t $$. </p> <p>Now, let’s look back at each term of the right hand side of the above equation, the first term is the predicted \(x_0\) given \(x_t\). The second term can be interpreted as the direction pointing to \(x_t\), and the third term is random noise sampled from a normal distribution. With the above equation, we have two special cases depending on the value of \(\sigma_t\). First, when \(\sigma_t = \sqrt{\frac{1-\hat{\alpha}_{t-1}}{1-\hat{\alpha}_t}} \sqrt{\frac{1-\hat{\alpha}_t}{\hat{\alpha}_{t-1}}}\), the forward diffusion process actually becomes Markovian, which means that the sampling process naturally becomes DDPM as well. Second, like when \(\eta=0\) above, we see that when \(\sigma_t = 0\) for all timestep, we see that there is no stochasticity as there is no random noise added. With the exception for when \(t=1\), we see that the process is deterministic and therefore this is why samples generated are nearly identical or share the same high level features.</p> <p>Lastly, another important note to make is that with the above equation, we can see that with the same trained model \(\hat{\epsilon}_{\theta}(x_t,t)\), we can have two different sampling methods DDPM and DDIM, with DDIM usually being superior over DDPM. Therefore, with DDIM, we do not need to retrain the model, which gives us another reason to use DDIM over DDPM.</p> <p>To wrap up, the main advantages of DDIM over DDPM are:</p> <ol> <li><strong>Faster sampling</strong>: As mentioned above, DDIM is a non-Markovian process that enables sample generation with a much smaller timestep \(S\), where \(S&lt;T\) when \(T\) is the timestep required for DDPM. When the sampling trajectory \(S\) is much smaller than \(T\), we experience more computational efficiency at some cost of image generation quality.</li> <li><strong>Control of stochasticity</strong>: As mentioned above, when \(\eta=0\) or \(\sigma_t=0\) for all timesteps, DDIMs are deterministic, meaning that if we start with the same latent vector (predicted noise) \(x_T\) via same random seed during sampling, the samples generated will all have the same high-level features.</li> <li><strong>Allows interpolation</strong>: In DDPMs, interpolation is still possible, but the interpolation will not be accurate due to the stochasticity in the samples generated from DDPM. However, utilizing deterministic samples from DDIM allows us to not only generate our samples quickly, but also be able to interpolate between two different domains easily.</li> </ol> <p>Therefore, we see that DDIM is a more efficient and effective sampling procedure over DDPM, and <strong>therefore, this is why the authors of the LDM paper use DDIM over DDPM.</strong> In the next and final part of the blog, we will cover conditioning and classifier/classifier-free guidance!</p> <hr/> <p><em>Image credits to:</em></p> <ul> <li><a href="https://arxiv.org/pdf/2006.11239.pdf">DDPM Training and Sampling Algorithm</a></li> <li><a href="https://arxiv.org/pdf/2010.02502.pdf">DDIM Forward/Sampling Diagram</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="concept-review"/><category term="generative-model"/><summary type="html"><![CDATA[Different interpretation of the training objective, explanation of training/sampling algorithm and mathematical comparison of DDIM and DDPM sampling methods.]]></summary></entry><entry><title type="html">(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 3</title><link href="https://chokevin8.github.io/blog/2023/stable-diffusion-part3/" rel="alternate" type="text/html" title="(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 3"/><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>https://chokevin8.github.io/blog/2023/stable-diffusion-part3</id><content type="html" xml:base="https://chokevin8.github.io/blog/2023/stable-diffusion-part3/"><![CDATA[<hr/> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <h3 id="latentstable-diffusion-fully-explained-part-1"><a href="/blog/2023/stable-diffusion/">Latent/Stable Diffusion Fully Explained! (Part 1)</a></h3> <ul> <li> <h3 id="introduction">Introduction</h3> </li> <li> <h3 id="stable-diffusion-vs-gan">Stable Diffusion vs GAN</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-2"><a href="/blog/2023/stable-diffusion-part2/">Latent/Stable Diffusion Fully Explained! (Part 2)</a></h3> <ul> <li> <h3 id="motivation">Motivation</h3> </li> <li> <h3 id="model-architecture">Model Architecture</h3> </li> <li> <h3 id="experiments--results">Experiments &amp; Results</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-3-this-blog"><a href="#stable-diffusion-in-numbers-1">Latent/Stable Diffusion Fully Explained! (Part 3)</a> (This Blog!)</h3> <ul> <li> <h3 id="vaes-and-elbo"><a href="#vaes-elbo">VAEs and ELBO</a></h3> </li> <li> <h3 id="model-objective"><a href="#model-objective-1">Model Objective</a></h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-4"><a href="/blog/2023/stable-diffusion-part4/">Latent/Stable Diffusion Fully Explained! (Part 4)</a></h3> <ul> <li> <h3 id="different-view-on-model-objective">Different View on Model Objective</h3> </li> <li> <h3 id="training-and-inference-ddim-vs-ddpm">Training and Inference (DDIM vs DDPM)</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-5--coming-soon"><a href="/blog/2023/stable-diffusion-part5/">Latent/Stable Diffusion Fully Explained! (Part 5- Coming Soon!)</a></h3> <ul> <li> <h3 id="conditioning">Conditioning</h3> </li> <li> <h3 id="classifier-free-guidance">Classifier-Free Guidance</h3> </li> <li> <h3 id="summary">Summary</h3> </li> </ul> <hr/> <p><em>Note: For other parts, please click the link above in the table of contents.</em></p> <p><a id="stable-diffusion-in-numbers-1"></a></p> <h2 id="stable-diffusion-in-numbers"><strong>Stable Diffusion In Numbers</strong></h2> <p>In this part of the blog, I will cover the mathematical details behind latent diffusion that is necessary to fully understand how latent diffusion works. This one took longer to write, as it is mathematically heavy, but it will be eventually worth it since understanding the underlying math will allow us to really fully understand stable diffusion. Before looking at the model objective of LDMs, I think it’s important to do an in-depth review on VAEs and how the Evidence Lower Bound (ELBO) is utilized:</p> <p><a id="vaes-elbo"></a></p> <h3 id="vaes-and-elbo-1"><strong><em>VAEs and ELBO:</em></strong></h3> <p>Let’s look at variational autoencoders (VAEs) in a probabilistic way. The variational autoencoder holds a probability model with the \(x\) representing the data, and the \(z\) representing the latent variables of the autoencoder. Remember that we want our latent variable \(z\) to model the data \(x\) as accurately as possible. Note that \(x\) can be seen, but \(z\) cannot since it is in the latent space. To perform the generative process, or run inference, for each individual data \(j\), we first sample latent variable \(z_i\) from the prior \(P(z)\): \(z_i \sim P(z)\). Then, with the prior sampled, we sample an individual data \(x_i\) from the likelihood \(P(x | z)\): \(x_i \sim P(x | z)\). Precisely, this can be represented in a graphical model below where we can see that the observed data \(x\) is conditioned on unobserved latent variable \(z\).</p> <p><img src="/assets/images/VAE_graphical_model.PNG" width="400" height="420" class="center"/></p> <figcaption>Diagram showing directed graphical model for VAEs.</figcaption> <p><br/> Now, remember again our goal in running inference in the VAE model is to model the latent space as good as possible given our data. This is <em>Bayesian Inference</em>, as “inference” means calculating the posterior probability, in this case the \(P(z | x)\). How do we calculate this? Let’s look at the classic Baye’s Rule:</p> <p> $$P(z | x) = \frac{P(x | z)\cdot P(z)}{P(x)}$$ </p> <p>In this case, each variable is: <br/> \(P(z)\) is the <strong><em>prior</em></strong> probability of \(z\), which is the initial belief without any knowledge about \(x\). <br/> \(P(x)\) is the <strong><em>evidence, or the marginal likelihood</em></strong>, the probability of observing \(x\) across all possible events. <br/> \(P(z | x)\) is the <strong><em>posterior</em></strong> probability of \(z\) given \(x\). <br/> \(P(x | z)\) is the <strong><em>likelihood</em></strong> of observing \(x\) given \(z\), which assumes the prior is correct.</p> <p>From above, let’s focus on the evidence, or the marginal likelihood. \(P(x)\) can be calculated by: \(P(x) = \displaystyle \int P(x | z)P(z) dz\) since we have a continuous distribution (in VAEs, the latent variable z is assumed to specified to be a Gaussian distribution with a mean of zero and unit variance (\(\mathcal{N}(0, 1)\)). However, this simple-looking integral over the product of gaussian conditional and prior distribution is <strong><em>intractable</em></strong> because the integration is performed over the entire latent space, which is continuous (it is possible to have infinite number of latent variables for a single input).</p> <p>But can we try calculating \(P(x)\) in a different way? We also know that the <em>joint probability</em> \(P(x,z) = P(x)P(z | x)\), meaning that \(P(x) = \frac{P(x,z)}{P(z | x)}\). We quickly realize that this doesn’t work either since we already saw above that the posterior \(P(z | x)\) is unknown! Therefore, we have to resort to approximating the posterior \(P(z | x)\) with an <em>approximate variational distribution \(q_\phi(z | x)\)</em> which has parameters \(\phi\) that needs to be optimized. Hence, in the previous graphical model, the dashed arrow going from x to z represents the variational approximation.</p> <p>But <em>how do we ensure that our approximate variational distribution \(q_\phi(z | x)\) will be as similar as possible to the intractable posterior \(P(z | x)\)?</em> We do this by minimizing the KL-divergence between the two distributions. For two distributions P and Q, KL-divergence essentially measures the difference between the two distributions. The value of the KL-divergence cannot be less than zero, as zero denotes that the two distributions are perfectly equal to each other. Note that \(D_{KL}(P || Q) = \sum_{n=i} P(i) \log \frac{P(i)}{Q(i)}\) <br/> Now let’s expand on this:</p> <p> $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(z|x)}{q(z|x)}$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)P(x)} \quad \text{since} \ P(z|x) = \frac {P(x,z)}{P(x)}$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) [\log \frac{P(x,z)}{q(z|x)} - \log P(x)]$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)} + \sum_{n=i} q(z|x) \log P(x)$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)} + \log P(x) \quad \text{since} \ \sum_{n=i} q(z|x) = 1$$ $$\log P(x) = min(D_{KL}(q(z|x) || P(z|x))) + \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)} \quad (1)$$ </p> <p>Observe the left hand side of the last line above (or equation #1). Since we observe \(x\), this is simply a constant. Observe the right hand side now. The first term is what we initially wanted to minimize. The second term is what’s called an <strong><em>“evidence/variational lower bound”, or “ELBO” for short</em></strong>. This is because rather than minimizing our first term (KL-divergence), utilizing the fact that the KL-divergence is always greater than or equal to zero, we can rather maximize the ELBO instead. We know understand why the second term is called a “lower bound”, as \(ELBO \leq \log P(x)\) since \(D_{KL}(q(z|x) || P(z|x)) \geq 0\) all the time. <br/> Now let’s expand on ELBO:</p> <p> $$ ELBO = \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)}$$ $$ ELBO = \sum_{n=i} q(z|x) \log \frac{P(x|z)P(z)}{q(z|x)}$$ $$ ELBO = \sum_{n=i} q(z|x) [\log P(x|z) + \log \frac{P(z)}{q(z|x)}]$$ $$ ELBO = \sum_{n=i} q(z|x) \log P(x|z) + \sum_{n=i} q(z|x) \log \frac{P(z)}{q(z|x)}$$ $$ ELBO = \sum_{n=i} q(z|x) \log P(x|z) - D_{KL}(q(z|x) || P(z))$$ $$ ELBO = \mathbb{E}_{q(z|x)} [\log P(x|z)] - D_{KL}(q(z|x) || P(z)) \quad (2)$$ </p> <p><strong><em>Remember</em></strong> the last line above (or equation #2) for later, this is also the <strong><em>loss function</em></strong> for training VAEs. But to understand this expression better, let’s now look at VAEs in a <em>neural network’s perspective</em>. A VAE consists of an encoder and a decoder, and both are neural networks. The <em>encoder</em> takes in input data \(x\) and compresses it to latent representation \(z\), and must learn a good latent representation known as the bottleneck of the model. Note that contrary to the encoder of the vanilla autoencoder, the encoder of the variational autoencoder will learn the mean and variance Therefore, the encoder can be denoted as \(q_\phi(z | x)\), where the \(\phi\) is the weights and biases of the model. Note that as previously mentioned, the latent space is assumed to be a Gaussian probability distribution, so sampling from the trained encoder gets us the latent representation \(z\) from data \(x\). The <em>decoder</em> takes in the latent representation <strong>z</strong> from the encoder output and outputs the reconstructed data denoted as \(\hat{x}\), or the parameters to the modeled probability distribution of the data space, and therefore can be denoted as \(p_\theta(x | z)\), where \(\theta\) is also the weights and biases. The below diagram helps us see this entire scheme.</p> <p><img src="/assets/images/autoencoder_diagram.png" width="800" height="420" class="center"/></p> <figcaption>Diagram showing autoencoder architecture.</figcaption> <p><br/> Now let’s go back to above equation #2. Let’s look at the first term \(\mathbb{E}_{q(z|x)} [\log P(x|z)]\). Now, remember that the latent space \(z\) is assumed to be a Gaussian distribution \(z_i \sim \mathcal{N}(0,1)\). Observe this:</p> <p> $$\log p(x|z) \sim \log exp(-(x-f(z))^2$$ $$\sim |x-f(z)|^2 $$ $$ = |x-\hat{x}|^2 $$ </p> <p>where \(f(z) = \hat{x}\), as the reconstructed image \(\hat{x}\) is the distribution mean \(f(z)\). This is because \(p(x|z) \sim \mathcal{N}(f(z), I)\). Therefore, here we see that the first term is correlated to the mean squared error (MSE) loss between the original image and the reconstructed image. This makes sense, as during training, we want to make penalize the model if the reconstructed image is too dissimilar to the original image. It is important to see that this was the first term of the <em>ELBO</em>, and remember we want to maximize this. Maximizing the first term is then therefore correlated to minimizing the MSE/reconstruction loss.</p> <p>Let’s now look at the second term, \(-D_{KL}(q(z|x) || P(z))\) (note the negative sign) which is the KL-divergence between our learned gaussian distribution (encoder) \(q(z|x)\) and the prior (latent space) gaussian distribution. Remember this is the second term of ELBO, so we still want to maximize- but note the negative sign, we actually want to minimize the KL divergence between the two- which makes sense as we want to encourage the learned distribution from the encoder to be similar to the unit Gaussian prior.</p> <p><img src="/assets/images/VAE_problem.png" width="800" height="400" class="center"/></p> <figcaption>Diagram showing VAE latent space with KL-regularization (left) and without KL-regularization (right).</figcaption> <p><br/></p> <p>This actually ties back to the KL-regularization of LDMs in the previous blog (part 2), which is the diagram showing the VAE latent space with and without KL-regularization. This is re-shown above. The minimization of KL divergence shown above regularizes the latent space as the “clusters” itself are bigger and are more centered around within each other. This ensures that the decoder creates <i>diverse and accurate samples</i>, as there would be smoother transitions between different classes (clusters). This is why both reconstruction loss term and KL-divergence term are included in the VAE loss function during training.</p> <p><img src="/assets/images/mnist_latent_space.jpg" width="600" height="600" class="center"/></p> <figcaption>Diagram showing regularized VAE latent space of MNIST dataset.</figcaption> <p><br/> For example, as seen above for MNIST handwritten digits, we see that the classes, or clusters have a smooth transition in this latent space. Without regularization, the encoder can cheat by learning narrow distributions with low variances. Now that we’ve understood the importance of maximizing ELBO to train a VAE, let’s go back to LDMs.</p> <hr/> <p><a id="model-objective-1"></a></p> <h3 id="model-objective-1"><strong><em>Model Objective:</em></strong></h3> <p>Now why did we go over the VAEs and its variational approximation process? This is because diffusion models have a very similar set up to VAEs in that it also has a tractable likelihood that can be maximized by maximizing the ELBO to ensure that the approximate posterior is as similar as possible to the unknown true posterior we’d like to model. We’re going to derive the training loss function, or the <em>model objective</em> just like how it was done for VAEs.</p> <p>Let’s first look at the forward and the backward diffusion process in a probabilistic way, since we already know about the diffusion processes in neural networks (train a regularized autoencoder for forward and backward diffusion process!). Take a look at the graphical model below:</p> <p><img src="/assets/images/forward_backward_diffusion.png" width="929" height="181" class="center"/></p> <figcaption>Graphical model showing the forward and reverse diffusion process.</figcaption> <p><br/> The forward diffusion process actually is the reverse of the above diagram, as the arrows should be going the opposite way- the forward diffusion process adds noise to a specific data point \(x_0\) that is sampled from the unknown, true distribution we’d like to model. Then, \(x_0\) has Gaussian noise added to it in a Markovian process (from \(x_{t-1}\) all the way to \(x_T\)) with \(T\) steps. Therefore, \(q(x_t|x_{t-1})\) takes the image and outputs a slightly more noisy version of the image. This can be formulated below:</p> <p> $$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \mu_t = \sqrt{1-\beta_t}x_{t-1},\Sigma_t = \beta_tI) \quad (3)$$ </p> <p>Assuming high-dimensionality, \(q(x_t|x_{t-1})\) is a Gaussian distribution with the above defined mean and variance. Note that for each dimension, it has the same variance \(\beta_t\). \(\beta_t\) is a number between 0 and 1, and essentially scales the data so the variance doesn’t grow out of proportion. The authors use a <em>linear schedule</em> for \(\beta_t\), meaning that \(\beta_t\) is linearly increased as the image gets noised more. Note that with above formula, we can easily obtain desired noised image at timestep \(T\) by using the Markovian nature of the process. Below is a tractable, closed-form formula to sample a noised image at any timestep:</p> <p> $$q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1}) \quad (4)$$ </p> <p>Basically, if T = 200 timesteps, we would have 200 products to sample the noised image \(x_{t=200}\). However, if the timestep gets larger, we run in to trouble of computational issues. Therefore, we utilize the <em>reparametrization trick</em> which gives us a much simpler tractable, closed-form formula for sampling that requires much fewer computations.</p> <p>The reparametrization trick is used whenever we sample from a distribution (Gaussian in our case) that is not directly differentiable. For our case, the mean and the variance of the distribution are both dependent on the model parameters, which is learned through SGD (as shown above). The issue is that because sampling from the Gaussian distribution is stochastic, we cannot compute the gradient anymore to update the mean and variance parameters. So, we introduce the auxiliary random variable \(\epsilon\) that is deterministic since it is sampled from a fixed standard Gaussian distribution (\(\epsilon \sim \mathcal{N}(0, 1)\)), which allows SGD to be possible since \(\epsilon\) is not dependent on the model parameters. Therefore, the reparametrization trick \(x = \mu + \sigma * \epsilon\) works by initially computing the mean and standard deviation using current weights given input data, then drawing deterministic random variable \(\epsilon\) to obtain the desired sample \(x\). Then, loss can be computed with respect to mean and variance, and they can be backpropagated via SGD.</p> <p> $$ \text{Let} \quad \alpha_t = 1 - \beta_t \text{and} \quad \hat{\alpha}_t = \prod_{i=1}^{t} \alpha_i $$ $$ \text{Also sample noise} \quad \epsilon_0, ..., \epsilon_{t-1} \sim \mathcal{N}(0,I) $$ $$ \text{Then,} \quad x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon_{t-1} $$ $$ x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1} $$ $$ x_t = \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}}\epsilon_{t-2} $$ $$ x_t = \quad ... $$ $$ x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}{\epsilon}_0 $$ $$ \mathbf{ \text{Therefore, since } \quad \mu + \sigma * \epsilon, \quad q(x_t|x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha}_t}x_0,\Sigma_t = (1-\hat{\alpha}_t)I)} \quad (5)$$ </p> <p><em>Note that above simplification is possible since the variance of two merged Gaussians is simply the sum of the two variances.</em></p> <p>To summarize the forward diffusion process, we can think of this as the encoder (remember encoder performs forward diffusion process to map pixel space to latent space) Each time step or each encoder transition is denoted as \(q(x_t|x_{t-1})\) which is from a fixed parameter \(\mathcal{N}(x_t,\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)\). Note that like VAE encoder, the encoder distribution for the forward diffusion process is also modeled as multivariate Gaussian. However, in VAEs, we learn the mean and variance parameters, while forward diffusion has fixed specific means and variances at each timestep as seen in equation #4.</p> <p>Now, look at the graphical model again for the reverse diffusion process, which is denoted by \(p_\theta(x_{t-1}|x_t)\). Now, if we could reverse the above forward diffusion process \(q(x_t|x_{t-1})\) and sample from \(q(x_{t-1}|x_t)\), we can easily run inference by sampling from our Gaussian noise input which is \(\sim \mathcal{N}(0,I)\). However, <em>this is exactly the same problem we had for VAEs as above !</em> This true posterior is unknown and is intractable since we have to compute the entire data distribution or marginal likelihood/evidence, \(q(x)\). Here, we can treat \(x_0\) as the true data, and every subsequent node in the Markovian chain \(x_1,x_2...x_T\) as a latent variable. Therefore, we approach this problem the exact same way.</p> <p>We approximate the true posterior \(q(x_{t-1}|x_t)\) with a neural network or a “decoder” that has parameters \(\theta\) (Note that this denoising diffusion “decoder” is the UNet, please don’t confuse this to the decoder of the autoencoder, which is something completely different as it is responsible for bringing the final output of \(x_0\) back to the pixel space. As previously discussed, we utilize UNet because of its inductive bias to images and its compatibility with cross attention). Like the forward process, but just reversing the timestep, we have:</p> <p> $$ p_{\theta}(x_{0:T}) = p(x_T) \prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) \quad (6)$$ </p> <p>This is just like the forward process in equation #4 but in reverse. By applying this formula, we can go from pure noise \(x_T\) to the approximated data distribution. Remember the encoder does not have learnable parameters (pre-defined or fixed), so we only need to train the decoder in learning the conditionals \(p_{\theta}(x_{t-1}|x_t)\) so we can generate new data. Conditioning on the previous timestep \(t\) and previous latent \(x_t\) lets the decoder learn the Gaussian parameters \(\theta\) which is the mean and variance \(\mu_{\theta},\Sigma_{\theta}\) (Since we assume variance is fixed due to noise schedule \(\beta\), however, we just need to learn the mean with the decoder). Therefore, running inference on a LDM only requires the decoder as we sample from pure Gaussian noise \(p(x_T)\) and run T timesteps of the decoder transition \(p_{\theta}(x_{t-1}|x_t)\) to generate new data sample \(x_0\). If our approximated \(p_{\theta}(x_{t-1}|x_t)\) steps are similar to unknown, true posterior steps \(q(x_{t-1}|x_t)\), the generated sample \(x_0\) will be similar to the one sampled from the training data distribution.</p> <p><em>Therefore, we want to train our decoder to find the reverse Markov transitions that will maximize the likelihood of the training data. Now, how do we train this denoising/reverse diffusion model?</em> We utilize the <strong><em>ELBO</em></strong> again. Remember for equation #1, we saw that the VAE was optimized by maximizing the ELBO (which was essentially the same as minimizing the negative log likelihood), we do the same below. Like VAEs, we first want to minimize the KL-divergence between the true unknown posterior \(q(x_{1:T}|x_0)\) and the approximated posterior \(p_{\theta}(x_{1:T}|x_0)\):</p> <p> $$ 0 \leq min \ D_{KL}(q(x_{1:T}|x_0)||p_{\theta}(x_{1:T}|x_0)) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log (p_{\theta}(x_0)) + min \ D_{KL}(q(x_{1:T}|x_0)||p_{\theta}(x_{1:T}|x_0)) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log (p_{\theta}(x_0)) + \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)})$$ $$ - \log (p_{\theta}(x_0)) \leq - \log (p_{\theta}(x_0)) + \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}) + \log(p_{\theta}(x_0)) \quad \text{since} \ p_{\theta}(x_{1:T}|x_0) = \frac{p_{\theta}(x_0|x_{1:T})p_{\theta}(x_{1:T})}{p_{\theta}(x_0)} = \frac{p_{\theta}(x_0,x_{1:T})}{p_{\theta}(x_0)} = \frac{p_{\theta}(x_{0:T})}{p_{\theta}(x_0)}$$ $$ - \log (p_{\theta}(x_0)) \leq \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}) \quad \text{since} \ - \log (p_{\theta}(x_0)) + \log(p_{\theta}(x_0)) = 0 \quad (7)$$ </p> <p>As seen in above, minimizing the KL-divergence also gives us the form \(- \log P(x) \leq ELBO\) or \(-ELBO \leq \log P(x)\), as we saw for VAEs in equation #1, since the RHS of equation #7 above is the <strong><em>ELBO</em></strong> for LDMs (\(ELBO_{LDM} = \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T}})\)). Therefore, instead of minimizing the above KL-divergence, we can maximize the ELBO like VAEs:</p> <p> $$ - \log (p_{\theta}(x_0)) \leq \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}) $$ $$ - \log (p_{\theta}(x_0)) \leq \log(\frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{p(x_T) \prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_t)}) \quad \text{since} \ p_{\theta}(x_{0:T}) = p(x_T) \prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t)$$ $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \log (\frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_t)}) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=1}^{T} \log(\frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}) + \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \quad \text{since} \ \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \ \text{is when} \ t = 1 \quad (8)$$ </p> <p>Note equation #8 above, and focus on \(q(x_t|x_{t-1})\) term. This is essentially the reverse diffusion step, but it is only conditioned on the pure Gaussian noise. The latent image vector \(x_{t-1}\) thus has a high variance, but by also conditioning on original image \(x_0\), we can decrease the variance and therefore enhance the image generation quality (think about it, if model is conditioned only on pure Gaussian noise, the produced latent image would vary more than the model conditioned on pure Gaussian noise <em>and</em> the original image as well). This is achieved by using the Baye’s rule:</p> <p> $$q(x_t|x_{t-1}) = \frac{q(x_{t-1}|x_t)q(x_t)}{q(x_{t-1})} = \frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}$$ </p> <p>Substituting this to equation #8 gives equation #9:</p> <p> $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{p_{\theta}(x_{t-1}|x_t)q(x_{t-1}|x_0)}) + \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \quad (9)$$ </p> <p>For equation #9, we can further split the second term on the RHS (the summation term) to two different summation terms to further simplify the RHS:</p> <p> $$ \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{p_{\theta}(x_{t-1}|x_t)q(x_{t-1}|x_0)}) = \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \sum_{t=2}^{T} \log(\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)})$$ </p> <p>Examining \(\sum_{t=2}^{T} \log(\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)})\), for any \(t&gt;2\), we see that all the terms in the denominator and numerator will cancel out each other and will simplify to \(\log(\frac{q(x_t|x_0)}{q(x_1|x_0)})\). <br/> Performing all of these substitutions to equation #9 gives equation #10:</p> <p> $$- \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \log(\frac{q(x_t|x_0)}{q(x_1|x_0)}) + \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \quad (10)$$ </p> <p>Now take the last two terms of the RHS in equation #10 above and further simplify by expanding the log:</p> <p> $$- \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \log(q(x_t|x_0)) - \log(q(x_1|x_0)) + \log(q(x_1|x_0)) - \log(p_{\theta}(x_0|x_1))$$ $$- \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \log(q(x_t|x_0)) - \log(p_{\theta}(x_0|x_1))$$ $$- \log (p_{\theta}(x_0)) \leq \log(\frac{q(x_t|x_0)}{p(x_T)}) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) - \log(p_{\theta}(x_0|x_1))$$ $$- \log (p_{\theta}(x_0)) \leq D_{KL}(q(x_T|x_0)||p(x_T)) + \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t)) - \log(p_{\theta}(x_0|x_1)) \quad (11)$$ </p> <p>Now look at equation #11 above, which is simplified further thanks to the definition of KL-divergence. The first RHS term \(D_{KL}(q(x_T|x_0)||p(x_T))\) has no learnable parameters, as we previously talked about the encoder \(q(x_T|x_0)\) having no learnable parameters as the forward diffusion process is fixed by the noising schedule shown in equation #3 and #5. Additionally, \(p(x_T)\) is just pure Gaussian noise as well. Lastly, it is safe to assume that this term will be zero, as q will resemble p’s random Gaussian noise and bring the KL-divergence to zero. Therefore, below is our final training objective, all we need to do is minimize the RHS of the equation:</p> <p> $$ - \log (p_{\theta}(x_0)) \leq \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t)) - \log(p_{\theta}(x_0|x_1)) \quad (12)$$ </p> <p>Now, to minimize the RHS of the equation our only choice is to minimize the first term \(\sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))\). Before diving into the derivation, let’s look at what this term actually means- it is the KL divergence between the ground truth denoising transition step \(q(x_{t-1}|x_t,x_0)\) and our approximation of the denoising transition step \(p_{\theta}(x_{t-1}|x_t)\), and it makes sense we want to minimize this KL divergence since we want the approximated denoising transition step to be as similar to the ground truth denoising transition step as possible.</p> <p>Utilizing Baye’s Rule, we can calculate the desired ground truth denoising step \(q(x_{t-1}|x_t,x_0)\) :</p> <p> $$ q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \quad (13) $$ </p> <p>Now, we know the form of the distribution of the denominator of equation #13 above, which is \(q(x_t|x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha_t}}x_0,\Sigma_t = (1-\hat{\alpha_t})I)\) Recall that this is from equation #5 from above and this was the reparametrization trick for the simplification of the forward diffusion process, or \(q(x_t|x_0)\) : \(x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}\epsilon\).</p> <p>Now, how about the numerator? We also know the forms of the two distributions in the numerator of equation #1 above as well. \(q(x_t \mid x_{t-1},x_0)\)is the forward diffusion noising step and is formulated in equation #3 above \(q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \mu_t = \sqrt{1-\beta_t}x_{t-1},\Sigma_t = \beta_tI) = q(x_t \mid x_{t-1}, x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\alpha_t}x_{t-1},\Sigma_t = (1-\alpha_t)I)\) where \(\alpha_t = 1-\beta_t\). The other distribution \(q(x_{t-1} \mid x_0)\) is a slight modification of the distribution in the numerator \(q(x_t|x_0)\), with \(t\) being \(t-1\) instead, so this is formulated as:</p> <p> $$q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \mu_t = \sqrt{\hat{\alpha}_{t-1}}x_0,\Sigma_t = (1-\hat{\alpha}_{t-1})I)$$ </p> <p>Now inputting all three of these formulations in the Baye’s Rule above in equation #13 we get equation #14 below:</p> <p> $$q(x_{t-1} \mid x_t,x_0) = \frac{\mathcal{N}(x_t; \mu_t = \sqrt{\alpha_t}x_{t-1},\Sigma_t = (1-\alpha_t)I) \mathcal{N}(x_{t-1}; \mu_t = \sqrt{\hat{\alpha}_{t-1}}x_0,\Sigma_t = (1-\hat{\alpha}_{t-1})I)}{\mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha}_t}x_0,\Sigma_t = (1-\hat{\alpha}_t)I)} \quad (14)$$ </p> <p>Now, combining the three different Gaussian distributions above to get the mean and variance for the desired \(q(x_{t-1} \mid x_t,x_0)\) is a lot of computations to show in this blog. The full derivation, for those who are curious, can be found in this <a href="https://arxiv.org/pdf/2208.11970.pdf">link</a>, <em>exactly in page 12 from equation 71 to 84</em>. (I just feel like this derivation is just a bunch of reshuffling variables with algebra, so it is unnecessary to include in my blog) Finishing this derivation shows that our desired \(q(x_{t-1} \mid x_t,x_0)\) is also normally distributed with the below formulation:</p> <p> $$q(x_{t-1} \mid x_t,x_0) \sim \mathcal{N}(x_{t-1}; \mu_t = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}},\Sigma_t = \frac{(1-\alpha_t)(1-\hat{\alpha}_{t-1})}{(1-\hat{\alpha_t})}I \quad (15)$$ </p> <p>From above, we can see that the above approximate denoising transition \(q(x_{t-1} \mid x_t,x_0)\) has mean that is a function of \(x_t\) and \(x_0\) and therefore can be abbreviated as \(\mu_q(x_t,x_0)\), and has variance that is a function of \(t\) (naturally) and the \(\alpha\) coefficients and therefore can be abbreviated as \(\Sigma_q(t)\). Recall that these \(\alpha\) coefficients are fixed and known, so that at any time step \(t\), we know the variance.</p> <p>Now, back to equation #12 where we want to minimize the KL-divergence:</p> <p> $$ \mathop{\arg \min}\limits_{\theta} \quad D_{KL}(q(x_{t-1} \mid x_t,x_0)||p_{\theta}(x_{t-1} \mid x_t)) $$ </p> <p>Equation #15 above tells us the formulation for ground truth denoising transition step \(q(x_{t-1} \mid x_t,x_0)\) , and we know the formulation for our approximate denoising transition step \(p_{\theta}(x_{t-1} \mid x_t)\).</p> <p>What is the KL-divergence between two Gaussian distributions? It is:</p> <p> $$ D_{KL}(\mathcal{N}(x;\mu_x,\Sigma_x) || \mathcal{N}(y;\mu_y,\Sigma_y)) = \frac{1}{2} [ \log \frac{\Sigma_y}{\Sigma_x} - d + tr({\Sigma_y}^{-1}\Sigma_x) + (\mu_y - \mu_x) ^ {T} {\Sigma_y}^{-1} (\mu_y - \mu_x) ] $$ </p> <p>Applying this KL-divergence equation to equation #12 above is also just reshuffling algebra, which is shown in the same link as before, from equations 87 to 92. We can see that equation #12 is simplified to:</p> <p> $$ \mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{|| \mu_{\theta} - \mu_q ||}^{2}] \quad (16) $$ </p> <p>To explain equation #16 above, \(\mu_q\) is the mean of the ground truth denoising transition step \(q(x_{t-1} \mid x_t,x_0)\) and \(\mu_{\theta}\) is the mean of our desired approximate denoising transition step \(p_{\theta}(x_{t-1} \mid x_t)\). How do we get these two values? We calculated them at equation 15, we can just utilize the \(\mu_t\), it depends on \(x_0\) and \(x_t\)! But wait, while \(q(x_{t-1} \mid x_t,x_0)\) is dependent on \(x_0\) and \(x_t\), \(p_{\theta}(x_{t-1} \mid x_t)\) is only dependent on \(x_t\), but not \(x_0\)! Well this is exactly what we’re trying to do, our approximate denoising step \(\hat{x}_{\theta}(x_t,t)\) is parametrized by the neural network with \(\theta\) parameters, we predict the generated/original image \(x_0\) using noisy image \(x_t\) and time step \(t\)!</p> <p>We see why it’s important to do derivations, it exactly shows what the objective is here now: train a neural network that parametrizes \(\hat{x}_{\theta}(x_t,t)\) to predict \(x_0\) as accurately as possible to make our approximate denoising step as similar to the ground truth denoising step as possible! \(\mu_q\) and \(\mu_{\theta}\), using equation #15 is:</p> <p> $$\mu_q = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}}$$ $$\mu_{\theta} = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_{\theta}(x_t,t)}{1-\hat{\alpha_t}}$$ </p> <p>Note that the two are exactly the same except \(x_0\) is replaced with \(\hat{x}_{\theta}(x_t,t)\) as mentioned before. Finally, plugging these two into equation #16 allows us to find the training objective:</p> <p> $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \mu_{\theta} - \mu_q ||}^{2}] \quad $$ $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_{\theta}(x_t,t)}{1-\hat{\alpha_t}} - \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}} ||}^{2}]$$ The first term of each term is the same, so after eliminating those: $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \frac{\sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_{\theta}(x_t,t)}{1-\hat{\alpha_t}} - \frac{\sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}} ||}^{2}]$$ $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \frac{\sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)}{1-\hat{\alpha_t}}(\hat{x}_{\theta}(x_t,t)-x_0)||}^{2}]$$ $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} \frac{\hat{\alpha}_{t-1}(1-\alpha_t)^{2}}{(1-\hat{\alpha_t)^{2}}} [{||(\hat{x}_{\theta}(x_t,t)-x_0)||}^{2}] \quad (17)$$ </p> <p>Equation #17 is finally our training objective. <em>To summarize again, we are learning the parameters \({\theta}\) from training a neural network to predict the ground truth image \(x_0\) from noised version of the image \(x_t\).</em> What’s important to take away from this, however, is understanding that, ultimately, applying the same ELBO method to maximize ELBO led to minimizing the KL divergence between ground truth and approximate denoising transition step, and this happens to be a form of <strong><em>minimizing the mean-squared-error (MSE) between the two distributions</em></strong> as seen in equation #17 above. This is quite <em>fascinating</em>, as all of this derivation just boils down to a simple MSE-like loss function.</p> <p>Now, with the training objective derived, the training algorithms and sampling algorithms will be explained in the next two parts of this blog with more mathematical details on LDMs that were not covered yet, especially regarding training/inference algorithms and conditioning/classifier-free guidance.</p> <hr/> <p><em>Image credits to:</em></p> <ul> <li><a href="https://arxiv.org/pdf/1312.6114.pdf">VAE Directed Graphical Model</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/cvae">MNIST Latent Space Example</a></li> <li><a href="https://arxiv.org/pdf/2006.11239.pdf">Graphical Model of Diffusion</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="concept-review"/><category term="generative-model"/><summary type="html"><![CDATA[Full derivation of ELBO in VAEs, and using that to derive training objective of latent/stable diffusion from scratch!]]></summary></entry><entry><title type="html">(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 2</title><link href="https://chokevin8.github.io/blog/2023/stable-diffusion-part2/" rel="alternate" type="text/html" title="(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 2"/><published>2023-06-30T00:00:00+00:00</published><updated>2023-06-30T00:00:00+00:00</updated><id>https://chokevin8.github.io/blog/2023/stable-diffusion-part2</id><content type="html" xml:base="https://chokevin8.github.io/blog/2023/stable-diffusion-part2/"><![CDATA[<hr/> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <h3 id="latentstable-diffusion-fully-explained-part-1"><a href="/blog/2023/stable-diffusion/">Latent/Stable Diffusion Fully Explained! (Part 1)</a></h3> <ul> <li> <h3 id="introduction">Introduction</h3> </li> <li> <h3 id="why-ditch-gans-for-stable-diffusion">Why ditch GANs for Stable Diffusion?</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-2-this-blog"><a href="#stable-diffusion-in-words">Latent/Stable Diffusion Fully Explained! (Part 2)</a> (This Blog!)</h3> <ul> <li> <h3 id="motivation"><a href="#motivation">Motivation</a></h3> </li> <li> <h3 id="model-architecture"><a href="#model-architecture">Model Architecture</a></h3> </li> <li> <h3 id="experiments--results"><a href="#experiment-results">Experiments &amp; Results</a></h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-3"><a href="/blog/2023/stable-diffusion-part3/">Latent/Stable Diffusion Fully Explained! (Part 3)</a></h3> <ul> <li> <h3 id="vaes-and-elbo">VAEs and ELBO</h3> </li> <li> <h3 id="model-objective">Model Objective</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-4"><a href="/blog/2023/stable-diffusion-part4/">Latent/Stable Diffusion Fully Explained! (Part 4)</a></h3> <ul> <li> <h3 id="different-view-on-model-objective">Different View on Model Objective</h3> </li> <li> <h3 id="training-and-inference-ddim-vs-ddpm">Training and Inference (DDIM vs DDPM)</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-5--coming-soon"><a href="/blog/2023/stable-diffusion-part5/">Latent/Stable Diffusion Fully Explained! (Part 5- Coming Soon!)</a></h3> <ul> <li> <h3 id="conditioning">Conditioning</h3> </li> <li> <h3 id="classifier-free-guidance">Classifier-Free Guidance</h3> </li> <li> <h3 id="summary">Summary</h3> </li> </ul> <hr/> <p><em>Note: For other parts, please click the link above in the table of contents.</em></p> <p><a id="stable-diffusion-in-words"></a></p> <h2 id="stable-diffusion-in-words"><strong><em>Stable Diffusion In Words:</em></strong></h2> <p>In part 1, I’ve introduced the concept of generative models and the disadvantages of GANs, but that’s not all the motivation of the authors of the paper. Let’s first look at the author’s motivation of developing the latent diffusion model:</p> <p><a id="motivation"></a></p> <h3 id="motivation-1"><strong><em>Motivation:</em></strong></h3> <p> Diffusion models are a (explicit) likelihood-based model, which can be classified as similar types to an autoregressive or a normalizing flow model. Therefore, diffusion models tend to share similar disadvantages as autoregressive or normalizing-flow models, as it suffers from high computational cost due to the model spending a lot of its resources and times preserving the details of the data entirely in the pixel space, which are often times unnecessary. Therefore, the authors aim to combat this issue by enabling diffusion models to train in the latent space without loss of performance, which enables training on limited computational resources. </p> <p> Now, converting the input image to the latent space isn't an easy task, as it requires compressing the images without losing its perceptual and semantic details. <i><b>Perceptual image compression</b></i> is just like what it sounds- it aims to preserve the visual quality of an image by prioritizing the information that is most noticeable to human perception while compressing and removing parts of the image that is less sensitive to human perception. In most cases, high frequency components of the image, which tend to be rapid changes in the images like edges and fine patterns can be removed as human perception is more sensitive to changes in low frequency components of the image such as major shapes and structures. </p> <p> <i><b>Semantic image compression</b></i> is a bit different- it aims to preserve the high-level semantic information that is important for understanding the overall image content, such as the outlines and borders of key objects of the image. </p> <p>In stable diffusion, the authors perform perceptual and semantic compression in two distinct steps:</p> <ol> <li>The autoencoder (model architecture will be explained in the next section) converts and compresses the input image from the pixel space to the latent space without losing perceptual details. Essentially, the autoencoder performs the perceptual compression by removing high-frequency details.</li> <li>The pretrained encoder and the U-Net, which as a combination is responsible for the actual generation of images (reverse diffusion), learns the semantic composition of the data. Essentially, the pretrained encoder and the U-Net performs semantic compression by learning how to generate the high-level semantic information of the input image.</li> </ol> <p> The autoencoder that maps between the pixel and the latent space and performs perceptual compression is called the "latent diffusion model". However, it seems like the authors meant to say that if the entire model architecture contains this mapping autoencoder, it can be under the same class of latent diffusion models. The authors mention that this universal autoencoding stage is needed to be trained only once, and this autoencoder can be utilized in various different multi-modal tasks. </p> <p>Now, one of the biggest reasons why previous diffusion or likelihood-based models required such high computational cost was because the models spent so much time updating and calculating losses, gradients, and different weights of the backbone during training and inference on parts of the images that are not important perceptual details. <br/> <img src="/assets/images/distortion_vs_rate.png" width="800" height="500" class="center"/></p> <figcaption>Diagram showing the relationship between rate and distortion and its tradeoff.</figcaption> <p><br/></p> <p> As seen above in the graph from the paper, we see the rate-distortion tradeoff. <i><b>Distortion</b></i> can be thought of as the root-mean-squared error (RMSE) between the original input image and the final generated image from the decoder. The lower the distortion, the lower the root-mean-squared error between the original image and the generated image. One may erroneously assume that a low distortion would always mean good perceptual quality of the image, but this is actually the complete opposite- optimizing one will alway come at the expense of another. </p> <p> <i><b>Rate</b></i>, or bits per dimension or pixel, can be thought of as the amount of information. Therefore, higher the rate, the more "information" there is in the image. I believe that the diagram shows the progression of the reverse diffusion process, where it starts at high distortion and zero rate (completely noised image) at time T, and where it ends at low distortion and high rate (completely denoised image) at time 0. Thus, it makes sense why distortion would decrease when rate is increased, as shown in the graph. The graph above first shows the separately trained universal autoencoder for an effective mapping of the input image in the pixel space to the latent space. Therefore, this autoencoder allows the reverse diffusion process of the conditioned U-Net to focus on generating the semantic details for semantic compression. </p> <p> But one may ask, <i><b>why specifically use U-Net?</b></i> The authors believe that by using U-Net, they can utilize the inductive bias of U-Net to generate high quality images. This is indeed true, as U-Nets, like other convolution-based networks, naturally excel at capturing the spatial relationship of images and have unique advantages like translational invariance due to convolutions. </p> <p>Therefore, the authors state that their work offers three main advantages over other previous diffusion models or any general generative models:</p> <ul> <li> Training an autoencoder that maps pixel to latent space makes the forward and reverse diffusion process computationally efficient with minimal losses in perceptual quality. </li> <li> The autoencoder only needs to be trained once and then can be used for various downstream training of multiple, multi-modal generative models.</li> <li> By utilizing the image-specific inductive bias of U-Net, the model does not need aggressive compression of images which could deteriorate perceptual quality of generated images. </li> </ul> <p>Before looking at experiments and results the authors concluded to verify those claims above, let’s look at the overall model architecture first.</p> <hr/> <p><a id="model-architecture"></a></p> <h3 id="model-architecture-1"><strong>Model Architecture:</strong></h3> <p>Stable diffusion consists of three major components- <em>an autoencoder, a U-Net, and a pretrained encoder</em>. Each of the three components are critical and work together to work their magic. The entire model architecture and its three major components can be visualized in the below image:</p> <p><br/> <img src="/assets/images/stable-diffusion.png" width="800" height="500" class="center"/></p> <figcaption>Diagram showing the general model architecture of the stable (latent) diffusion.</figcaption> <ol> <li> <p><strong>Autoencoder:</strong> The autoencoder is responsible for two major tasks, with the encoder and the decoder being responsible for each task. First, the encoder allows the previously mentioned forward diffusion process to happen in the latent space. This means that the forward diffusion process, which is Markovian, would take less time since the image from our pixel space is essentially downsampled into the latent space. Without the encoder, the forward diffusion process in the pixel space would simply take too long. Likewise, the decoder is then responsible for upsampling the generated latent space image back to the pixel space. The generated latent space image is obtained from the output of the U-Net, which will be mentioned next. The decoder is needed because the generated latent space image needs to be converted back to the pixel space to obtain our final desired image. Basically, the autoencoder allows the forward and the backward diffusion process to happen in the latent space, and also performs perceptual compression by removing high-frequency details as explained in the previous section. Note that this autoencoder can be separately trained only once and be applied in various different tasks since the generative power of the model resides in the U-Net + pretrained encoder.</p> </li> <li> <p><strong>U-Net:</strong> The U-Net, which is a popular model used in semantic segmentation, is a U-shaped fully convolutional neural network. It follows a U-shape because of its downsampling contracting path with max-pooling and its upsampling expansive path, with skip connections between each layers to preserve semantic details. The U-Net in stable diffusion, is responsible for denoising the noisy latent vector that was produced by the encoder, meaning that it is responsible for the reverse diffusion (generation) process. Therefore, when we train a diffusion model, we’re essentially training the denoising U-Net. However, this denoising U-Net isn’t without additional help, as it is conditioned by not only the noisy latent vector, but also by the latent embeddings generated by the encoder via a cross-attention mechanism built in to the U-Net architecture. Since it is a cross-attention mechanism, these embeddings can be from different modalities such as text, image, semantic map, and more. The inductive bias of the U-Net giving it a natural advantage in handling image data with cross-attention in the backbone makes the generative process stable (as aggressive compression is not needed) and flexible (cross attention allows multi-modality).</p> </li> <li> <p><strong>Pretrained Encoder:</strong> <br/> Therefore, the pretrained text/image (mostly text and image modalities are used as prompts) encoder is responsible for projecting the conditioning text/image prompts to an intermediate representation that can be mapped to the cross-attention components, which is the usual query, key, and value matrices. For example, BERT or BERT-like pretrained text encoders have been pretrained on huge corpus datasets and are suited to take text prompts and generate token embeddings which are the intermediate representations that gets mapped to the cross-attention components. Nowadays, CLIP or CLIP-like pretrained text/image encoders pretrained on huge dataset of image-text pairs are used and thus allows text and image prompted stable diffusion as well (BERT can not handle images).</p> </li> </ol> <hr/> <p><a id="experiment-results"></a></p> <h3 id="experiments--results-1"><strong>Experiments &amp; Results:</strong></h3> <p>Now, with the above motivation resulting in the authors designing this unique model architecture, the authors performed several experiments to verify their claims.</p> <p><strong>1. Experiment on Perceptual Compression Tradeoffs:</strong></p> <p> Recall that the autoencoder is responsible for mapping the input image from the pixel space to the latent space and vice versa, and therefore needs an optimized downsampling factor for it to be effective- too high of a downsampling factor will be too aggressive in the perceptual compression and cause information loss and too low of a downsampling factor will make the training process slower since it would leave most of the perceptual compression to the reverse diffusion process (image not compressed enough). As expected, the graph below shows that a downsampling factor of 4 or 8 was the ideal factor for training the autoencoder. </p> <p><img src="/assets/images/optimizing_downsampling_factor.jpeg" width="1000" height="400" class="center"/></p> <figcaption>Diagram showing FID and Inception Scores of generated images for different downsampling factors of the autoencoder.</figcaption> <p><br/></p> <p> It is confirmed that LDM with downsampling factor 4 and 8 achieve the lowest FID score and the highest Inception Score, with downsampling factors at each extreme ends (1 and 32) performing poorly as expected. Lastly, the authors utilized LDM with downsampling factor of 4 and 8 and tested them against multiple benchmark datasets. It was concluded that LDM's did show SOTA performance compared to that of previous diffusion-based SOTA models in all but one dataset on FID, and also performed better than GANs on precision and recall. This improved performance was also with using significantly less computational resources, matching the author's hypothesis earlier. Please refer to the paper for more information on the results, as it is too much detail to cover every result in the blog. </p> <p><strong>2. Conditional Latent Diffusion:</strong></p> <p> Note that one of the biggest advantages to the LDM is its multi-modality due to its cross-attention based text/image conditioning made possible with the U-Net backbone and pretrained encoders like BERT and CLIP. The authors explore the multi-modality by performing different experiments in LDM's ability to perform semantic synthesis, image superresolution, and image inpainting. Before briefly looking over each experiment, however, it is important to touch upon: <i><b> 1) KL and VQ-regularized LDMs and 2) Classifier-guided and classifier-free diffusion process.</b></i> </p> <p>-<strong><em>KL and VQ-regularized LDMs:</em></strong> <br/> KL-regularization actually originates from variational autoencoders (VAEs), which is a type of autoencoder where its encodings are regularized so that the latent space can be sufficiently diverse, so that the resulting image generated from the decoder is diverse but also accurate. This regularization is needed because without regularization, VAEs will tend to spread out in clusters in the latent space, which deteriorates the decoder’s performance as it learns to just regurgitate the training data. This becomes more clear when looking at the below diagram:</p> <p><img src="/assets/images/VAE_problem.png" width="800" height="400" class="center"/></p> <figcaption>Diagram showing VAE latent space with KL-regularization (left) and without KL-regularization (right).</figcaption> <p><br/></p> <p> The left plot shows the VAE latent space with KL-regularization, as the clusters are closer together and each cluster has a wider "radius". This VAE will generate images that are both accurate and diverse. On the other hand, the right plot shows the VAE latent space without KL-regularization, as the clusters are further apart and each cluster has a narrower "radius". This VAE will not generate diverse images, and often times images may be "weird looking" since there is no smooth transition/interpolation between different classes (clusters are further apart). </p> <p> The "KL" stands for the <i>Kullback-Leibler (KL) divergence</i>, that is additionally added to the loss function of the VAEs. The KL-divergence essentially measures the distance between two probability distributions, and minimizing this essentially brings the above clusters "more together". More on VAEs and KL-divergence will be covered in the next part of the blog. </p> <p> VQ-regularization is another method to regularize the latent space of a VAE, a similar method is utilized in <i>Vector-Quantized (VQ) VAEs</i>- hence why it is called VQ-regularization. VQVAEs, unlike VAEs briefly described above, utilize discrete latent variables instead of a continuous normal distribution used in the original VAE. Then, the embeddings generated by the encoder is categorical, and samples drawn from this generate a discrete embedding dictionary. The authors call this regularization process with a <i>"vector quantization layer by learning a codebook of |Z| different exemplars"</i>, in which the vector quantization layer converts the continuous latent distribution to discrete indices. Here, "codebook" refers to the discrete embedding dictionary. To briefly explain how the vector quantization layer works, the vector quantization works by comparing the continuous input from the encoder and the "codebook" and finding the index of the closest vector ("argmin") in the "codebook" by using Euclidean distance or other similarity measures. Again, more on VQVAEs and VQ-regularization can be covered later, but is not the focus of this blog. The authors utilize the VQ-regularization for their LDMs in the decoder of the autoencoder in an attempt to regularize the latent space, in a way to enhance the interpretability of the latent space and hence increase the robustness and quality of the generated samples. </p> <p>-<strong><em>Classifier-guided and classifier-free diffusion process:</em></strong> <br/> In the reverse diffusion or sampling process, one can utilize a classifier-guided or a classifier-free diffusion process. A classifier-guided diffusion process, like its name, requires a separate classifier to be trained. This classifier guidance technique did boost the sample quality of a diffusion model using the separately trained classifier, by essentially mixing the score (score = gradient of log probability) of the diffusion model and the gradient of the log probability of this auxillary classifier model. However, not only was training this auxillary classifier time-consuming (it requires training on noisy images, meaning it cannot be pre-trained), but this process of mixing resembles an “adversarial attack” (adversarial attack meaning introducing slight perturbations the input which confuses the model and results in different outputs). Therefore, a classifier-free diffusion process is utilized by the authors, which doesn’t require a separate classifier to be trained, and still boosts the sample quality of the LDM. This classifier-free approach requires training a conditional and an unconditional diffusion model simultaneously, and mixes the two scores together. We will look at classifier-guidance in more detail in <a href="/blog/2023/stable-diffusion-part4/">Part 4 of the blog</a>.</p> <p> Now, to come back to the conditional LDMs, the authors wanted to test how their model performed on a text-to-image synthesis by using a BERT tokenizer. The authors concluded that their "LDM-KL-8-G" model, or their classifier-free, KL-regularized LDM with downsample factor of 8 performed on par with recent SOTA diffusion or autoregressive models despite utilizing significantly lower number of parameters. With their success, the authors tested their LDM model on four additional tasks: </p> <p>-<strong><em>Semantic Synthesis:</em></strong> Semantic synthesis was tested to see the LDM’s ability to condition on different modalities outside of text. As seen in the diagram below, a 256 x 256 resolution semantic map was conditioned on the LDM to generate a 512 x 1024 resolution landscape image. The authors tested various different downsampling factors and also both KL- and VQ-regularized LDMs, and concluded that signal-to-noise ratio (SNR) significantly affected the sample quality.</p> <p><img src="/assets/images/semantic_synthesis.png" width="800" height="400" class="center"/></p> <figcaption>Diagram showing a smaller resolution semantic map conditioned on the LDM and the resulting generated larger resolution landscape image (right).</figcaption> <p><br/></p> <p>The SNR was high for LDMs trained in the latent space regularized by KL-regularization as the variance was too high- resulting in low fidelity images. Therefore, KL-regularized LDMs had its latent space rescaled by its component-wise standard deviation of the latents and the SNR was decreased (VQ-regularized space doesn’t have this issue as VQ-regularized latent space has variance close to 1).</p> <p>-<strong><em>Image Super-resolution and Inpainting:</em></strong> Image super-resolution is to generate a higher resolution version of the input image, which is basically an advanced version of semantic synthesis. This was achieved by conditioning on low-resolution images as input, and those low-resolution images were initially degraded using bicubic interpolation with 4x downsampling. The LDM concatenates the low resolution conditioning and the inputs to the UNet, resulting in a “super-resolution” image. The authors were able to achieve SOTA performance, and they also developed a more general model that could handle different types of image degradation other than bicubic interpolation for robustness. Image inpainting is to fill in a masked region of a specific image. The authors also report SOTA performance on FID and noted that the VQ-regularized, 4x downsampled LDM-4 worked the best.</p> <hr/> <p>Most of the important parts of the paper has been covered, but there was barely any math in my explanations. Fully understanding stable diffusion without covering its mathematic details would not be possible. The next three parts (Parts 3, 4, and 5) will cover all of this.</p> <p><em>Image credits to:</em></p> <ul> <li><a href="https://towardsdatascience.com/what-are-stable-diffusion-models-and-why-are-they-a-step-forward-for-image-generation-aa1182801d46">Stable Diffusion Architecture</a></li> <li><a href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">VAE Latent Space KL-Regularization</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="concept-review"/><category term="generative-model"/><summary type="html"><![CDATA[Analysis of latent/stable diffusion model architecture, and explanation of major experiments/results of the paper!]]></summary></entry><entry><title type="html">(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 1</title><link href="https://chokevin8.github.io/blog/2023/stable-diffusion/" rel="alternate" type="text/html" title="(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 1"/><published>2023-05-20T00:00:00+00:00</published><updated>2023-05-20T00:00:00+00:00</updated><id>https://chokevin8.github.io/blog/2023/stable-diffusion</id><content type="html" xml:base="https://chokevin8.github.io/blog/2023/stable-diffusion/"><![CDATA[<blockquote> Welcome to my first blog post! From now on, I'll be trying to do regular updates on any interesting, recent AI-related topic. </blockquote> <p>For my first post, I thought it’d be fitting to do an in-depth review on the stable diffusion model which basically started the stable diffusion boom last year. It really is the “hot topic” right now, as the generative models are taking over the AI industry. For reference purposes, the stable diffusion paper that started it all is named “High-Resolution Image Synthesis with Latent Diffusion Models”, and can be found <a href="https://arxiv.org/pdf/2112.10752.pdf">here</a>. For this part (part 1), I will just touch upon the surface about stable diffusion and its predecessor, GANs. The next three parts, as seen in the table of contents below, will cover much more qualitative and quantitative details. <strong> Let’s dive right in!</strong></p> <p><img src="/assets/images/welcome-pikachu.png" width="400" height="400" class="center"/></p> <figcaption>Cute pikachu drawing made from DALLE-2!</figcaption> <hr/> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <h3 id="latentstable-diffusion-fully-explained-part-1-this-blog"><a href="#background">Latent/Stable Diffusion Fully Explained! (Part 1)</a> (This Blog!)</h3> <ul> <li> <h3 id="introduction"><a href="#introduction">Introduction</a></h3> </li> <li> <h3 id="why-ditch-gans-for-stable-diffusion"><a href="#why-ditch-GANs">Why ditch GANs for Stable Diffusion?</a></h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-2"><a href="/blog/2023/stable-diffusion-part2/">Latent/Stable Diffusion Fully Explained! (Part 2)</a></h3> <ul> <li> <h3 id="motivation">Motivation</h3> </li> <li> <h3 id="model-architecture">Model Architecture</h3> </li> <li> <h3 id="experiments--results">Experiments &amp; Results</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-3"><a href="/blog/2023/stable-diffusion-part3/">Latent/Stable Diffusion Fully Explained! (Part 3)</a></h3> <ul> <li> <h3 id="vaes-and-elbo">VAEs and ELBO</h3> </li> <li> <h3 id="model-objective">Model Objective</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-4"><a href="/blog/2023/stable-diffusion-part4/">Latent/Stable Diffusion Fully Explained! (Part 4)</a></h3> <ul> <li> <h3 id="different-view-on-model-objective">Different View on Model Objective</h3> </li> <li> <h3 id="training-and-inference-ddim-vs-ddpm">Training and Inference (DDIM vs DDPM)</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-5--coming-soon"><a href="/blog/2023/stable-diffusion-part5/">Latent/Stable Diffusion Fully Explained! (Part 5- Coming Soon!)</a></h3> <ul> <li> <h3 id="conditioning">Conditioning</h3> </li> <li> <h3 id="classifier-free-guidance">Classifier-Free Guidance</h3> </li> <li> <h3 id="summary">Summary</h3> </li> </ul> <hr/> <p><em>Note: For other parts, please click the link above in the table of contents.</em></p> <p><a id="background"></a></p> <h2 id="background"><strong>Background:</strong></h2> <p>Latent diffusion models (LD), or henceforth mentioned as stable diffusion models (SD), are a type of a diffusion model developed for the purpose of image synthesis by Rombach et al. of last year. (While LD and SD are not exactly equal- SD is an improved version and hence a type of LD- we will use the term interchangeably) Let’s do a quick review/introduction of what a “model” is first:</p> <p><a id="introduction"></a></p> <h3 id="introduction-1"><strong><em>Introduction:</em></strong></h3> <p>In Machine Learning, a model is either a generative or discriminative. The diagram below clearly shows the difference between the two: <br/> <img src="/assets/images/generative_v_discriminative.png" width="523" height="293" class="center"/></p> <figcaption>Diagram showing difference between generative and discriminative models.</figcaption> <p><br/> As shown above, the discriminative model tries to tell the difference of a writing of 0 and 1 by just simply drawing a decision boundary through the data space. Therefore, a discriminative model just has to model the posterior \(p(y|x)\) for label y and data sample x. It doesn’t need to model the probability distribution of the entire data space to do this. For example, if you are familiar with SVMs, which is a type of a discriminative classifier/model, we know that the model’s objective is to find the support vectors that maximize the distance between the support vectors and the decision boundary, which is the margin. Most of the time, the support vectors are very few data points that lie near the decision boundary (hyperplane)– the majority of the other data samples simply don’t matter.</p> <p>On the other hand, a generative model aims to model the probability distribution \(p(x,y)\) of the entire data space. For cases where there is no label (semi-supervised/unsupervised cases), a generative model aims to model \(p(x)\) instead. We can see in the diagram above that the generative model has come up with a probability distribution of the 0’s and the 1’s that well-represents the true, hidden probability distribution of the entire data space. Therefore, the generative model generally has to “work harder” to achieve its goals. However, this unique nature of generative models allows it to “generate” synthesized data by using sampling methods from the modeled probability distribution, just like the cute pikachu drawing I generated by using DALLE-2, which is a type of generative model! Likewise, a stable diffusion model is a type of generative model. Generative models can be broadly classified into four types:</p> <ol> <li><strong>Flow-based models:</strong> Flow-based generative models are quite unique in that they utilize a method called “normalizing flow”. Normalizing flow uses the change of variable theorem, which allows us to estimate a more complex probability density function for our model. This is hugely beneficial but also comes at a cost- during backpropagation, the derivative would be impossible or too hard to calculate. Therefore, we will later see that this is why stable diffusion utilizes the gaussian distribution in its noising process, even though it is much simpler than the “real world” distribution. Normalizing flow is essentially a complex distribution modeled by a chain of invertible transformation functions. The probability density function is tractable, meaning the learning process is simply based on minimizing the negative log-likelihood over the given dataset. A critical drawback, however, is that normalizing models have limitations in that the transformations must all be invertible (for change of variable theorem to work) and determinants must be efficiently calculated (for backpropagation).</li> <li><strong>Autoregressive models:</strong> Autoregressive generative models, like their name suggests, means performing regression on its self. General autoregression means predicting a future outcome based on the previous data of that outcome. The general idea is that autoregressive models model the joint probability space \(p(x)\) by utilizing the chain rule \(p(x,y) = p(y|x)p(x)\), meaning that it is ultimately a product of conditional distributions. Like normalizing flows, defining the complex product of conditional distribution is no easy task, and autoregressive models do this by utilizing the deep neural networks. In this case, outputs of the neural network is fed back as input, with the layers being one or more convolutional layers. Like normalizing flow models, the probability distribution is tractable, but the sampling process is slower as it is sequential by nature (sequential conditionals).</li> <li><strong>Generative Adversarial Networks (GAN):</strong> GANs will be covered in more detail in this blog.</li> <li><strong>Latent variable models:</strong> <strong>Stable diffusion models</strong> belong to this type. This will also be covered in more detail in this blog.</li> </ol> <hr/> <p><a id="why-ditch-GANs"></a></p> <h2 id="why-ditch-gans-for-stable-diffusion-1"><strong><em>Why ditch GANs for Stable Diffusion?</em></strong></h2> <p>Surprisingly, diffusion models are not new at all! Stable diffusion is a type of diffusion model, and like its name suggests, is actually based on the diffusion from thermodynamics! The forward diffusion process is where random (usually Gaussian) noise is introduced to an image until the image is pure noise (isotropic Gaussian), and the reverse diffusion process is where the model is trained so that the model is able to generate data samples from the noise that is representative of the true data distribution. Before diving deeper into stable diffusion, let’s first review GAN’s disadvantages, because before stable diffusion’s emergence as the SOTA (state-of-the-art) generative model, GAN and its variants have been the SOTA generative model (however GANs still may be superior in niche use cases since sampling is still much faster on GANs).</p> <p><br/> <img src="/assets/images/GAN_architecture.png" width="700" height="525" class="center"/></p> <figcaption>Diagram showing general GAN architecture.</figcaption> <p><br/></p> <p> The diagram above shows the general GAN architecture. As seen in the diagram, a GAN consists of a generator and a discriminator, which are both deep neural networks that are trained. The generator learns to generate plausible data, or so-called "fake images". The generated instances of fake images then become negative training examples for the discriminator. Then, the discriminator learns to distinguish the generator's fake data from real data. Hence, the discriminator is simply a classifier model where it labels the images generated by the generator as real or fake. Simple! </p> <p> The training process consists of the discriminator loss penalizing the discriminator for misclassifying the real image as fake or fake image as real. Then, the suffered loss is then used for backpropagation to update the discriminator’s weights. Next, the generator takes a random input of noise, which is a probability distribution (e.g. Gaussian) and generates the fake samples. The training process consists of the generator loss penalizing the generator for not being able to "trick" the discriminator. Likewise, the suffered loss is used for backpropagation to update the generator’s weights. Note that the discriminator and generator are separately trained, meaning that when the generator is trained the weights of the discriminator is fixed. </p> <p>The ideal situation is that if this is done repeatedly, the generator would eventually be able to learn the entire joint probability distribution of the desired data set. But how do we know if we’re done training? This is one of GAN’s biggest drawbacks, but generally, if the discriminator is starting to give completely random feedback, we know we’ve done well. This would mean that the generator is generating fake images that are so similar to the real images that the discriminator cannot distinguish them. Now let’s touch upon the critical disadvantages that GANs have due to their intrinsic architecture and training approach. <br/></p> <ol> <li>GANs are a type of implicit generative model, meaning it implicitly learns the joint probability distribution (probability density function is intractable), meaning that any modification or inversion of the model is difficult. (Explicit generative models like the flow-based and autoregressive models have tractable density functions.) This makes training unstable, as we cannot rely on the actual loss function of the GAN during the training process.</li> <li>Furthermore, because two separate networks must be trained, GANs suffer from high training time and will sometimes fail to converge if GAN continues training past the point when the discriminator is giving completely random feedback. In this case, the generator starts to train on junk feedback, and the generated image will suddenly start to degrade in quality.</li> <li>Also, if the generator happens to create a very plausible output, the generator in turn would learn to only produce that type of one output. If the discriminator then gets stuck in a local minima and it can’t find itself out, the generator and the entire model only generates a small subset of output types. This is a common problem in GANs called mode collapse.</li> <li>Lastly, we can have vanishing gradients when the discriminator performs very well, as there would be little loss suffered from the generator and hence almost no weight updated for the generator model through backpropagation.</li> </ol> <p> To address these issues during training, when training and evaluating GANs, researchers generally use both qualitative and quantitative metrics during the training and evaluation process. Qualitative metrics are essentially human judges rating the quality of the generated images compared to the ground-truth images. Quantitative metrics that are often used, are Inception Score (IS) and Frechet Inception Distance (FID). </p> <p><br/> <img src="/assets/images/gan_meme.jpeg" width="300" height="450" class="center"/></p> <figcaption>Assuming the GANs are well-trained, this meme above pretty much explains the life of a discriminator. How hard it must be!</figcaption> <p><br/></p> <hr/> <p>While the drawbacks of GANs listed above do have their own remedies, they may still not work, or even if they do work, they may require a lot of time and effort- which may not be worth it. However, stable diffusion hasn’t become the SOTA generative model just because of the drawbacks of GANs, they have their own advantages as well! The paper itself will be detailed in <a href="/blog/2023/stable-diffusion-part2/">part 2</a>.</p> <p><em>Image credits to:</em></p> <ul> <li><a href="https://developers.google.com/machine-learning/gan/generative">Discriminative Model vs Generative Model</a></li> <li><a href="https://developers.google.com/machine-learning/gan/generator">GAN Architecture</a></li> <li><a href="https://medium.com/@harikrishnareddy19995/gans-with-memes-4233952ba151">GAN Discriminator Meme</a></li> </ul>]]></content><author><name></name></author><category term="posts"/><category term="concept-review"/><category term="generative-model"/><summary type="html"><![CDATA[Introduction to generative models and drawbacks of GANs!]]></summary></entry></feed>