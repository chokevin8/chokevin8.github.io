<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 3 | Kevin (Won June) Cho</title> <meta name="author" content="Kevin (Won June) Cho"> <meta name="description" content="Full derivation of ELBO in VAEs, and using that to derive training objective of latent/stable diffusion from scratch!"> <meta name="keywords" content="deep-learning, image-segmentation, python, pytorch"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://chokevin8.github.io/blog/2023/stable-diffusion-part3/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Kevin (Won June) Cho</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">My Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">My Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">My Github</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">My Resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">(Concept Review/Generative Models) Latent/Stable Diffusion Fully Explained, Part 3</h1> <p class="post-meta">August 15, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/concept-review"> <i class="fas fa-hashtag fa-sm"></i> concept-review</a>   <a href="/blog/tag/generative-model"> <i class="fas fa-hashtag fa-sm"></i> generative-model</a>     ·   <a href="/blog/category/posts"> <i class="fas fa-tag fa-sm"></i> posts</a>   </p> </header> <article class="post-content"> <hr> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <h3 id="latentstable-diffusion-fully-explained-part-1"><a href="/blog/2023/stable-diffusion/">Latent/Stable Diffusion Fully Explained! (Part 1)</a></h3> <ul> <li> <h3 id="introduction">Introduction</h3> </li> <li> <h3 id="stable-diffusion-vs-gan">Stable Diffusion vs GAN</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-2"><a href="/blog/2023/stable-diffusion-part2/">Latent/Stable Diffusion Fully Explained! (Part 2)</a></h3> <ul> <li> <h3 id="motivation">Motivation</h3> </li> <li> <h3 id="model-architecture">Model Architecture</h3> </li> <li> <h3 id="experiments--results">Experiments &amp; Results</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-3-this-blog"> <a href="#stable-diffusion-in-numbers-1">Latent/Stable Diffusion Fully Explained! (Part 3)</a> (This Blog!)</h3> <ul> <li> <h3 id="vaes-and-elbo"><a href="#vaes-elbo">VAEs and ELBO</a></h3> </li> <li> <h3 id="model-objective"><a href="#model-objective-1">Model Objective</a></h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-4"><a href="/blog/2023/stable-diffusion-part4/">Latent/Stable Diffusion Fully Explained! (Part 4)</a></h3> <ul> <li> <h3 id="different-view-on-model-objective">Different View on Model Objective</h3> </li> <li> <h3 id="training-and-inference-ddim-vs-ddpm">Training and Inference (DDIM vs DDPM)</h3> </li> </ul> <h3 id="latentstable-diffusion-fully-explained-part-5--coming-soon"><a href="/blog/2023/stable-diffusion-part5/">Latent/Stable Diffusion Fully Explained! (Part 5- Coming Soon!)</a></h3> <ul> <li> <h3 id="conditioning">Conditioning</h3> </li> <li> <h3 id="classifier-free-guidance">Classifier-Free Guidance</h3> </li> <li> <h3 id="summary">Summary</h3> </li> </ul> <hr> <p><em>Note: For other parts, please click the link above in the table of contents.</em></p> <p><a id="stable-diffusion-in-numbers-1"></a></p> <h2 id="stable-diffusion-in-numbers"><strong>Stable Diffusion In Numbers</strong></h2> <p>In this part of the blog, I will cover the mathematical details behind latent diffusion that is necessary to fully understand how latent diffusion works. This one took longer to write, as it is mathematically heavy, but it will be eventually worth it since understanding the underlying math will allow us to really fully understand stable diffusion. Before looking at the model objective of LDMs, I think it’s important to do an in-depth review on VAEs and how the Evidence Lower Bound (ELBO) is utilized:</p> <p><a id="vaes-elbo"></a></p> <h3 id="vaes-and-elbo-1"><strong><em>VAEs and ELBO:</em></strong></h3> <p>Let’s look at variational autoencoders (VAEs) in a probabilistic way. The variational autoencoder holds a probability model with the \(x\) representing the data, and the \(z\) representing the latent variables of the autoencoder. Remember that we want our latent variable \(z\) to model the data \(x\) as accurately as possible. Note that \(x\) can be seen, but \(z\) cannot since it is in the latent space. To perform the generative process, or run inference, for each individual data \(j\), we first sample latent variable \(z_i\) from the prior \(P(z)\): \(z_i \sim P(z)\). Then, with the prior sampled, we sample an individual data \(x_i\) from the likelihood \(P(x | z)\): \(x_i \sim P(x | z)\). Precisely, this can be represented in a graphical model below where we can see that the observed data \(x\) is conditioned on unobserved latent variable \(z\).</p> <p><img src="/assets/images/VAE_graphical_model.PNG" width="400" height="420" class="center"></p> <figcaption>Diagram showing directed graphical model for VAEs.</figcaption> <p><br> Now, remember again our goal in running inference in the VAE model is to model the latent space as good as possible given our data. This is <em>Bayesian Inference</em>, as “inference” means calculating the posterior probability, in this case the \(P(z | x)\). How do we calculate this? Let’s look at the classic Baye’s Rule:</p> <p> $$P(z | x) = \frac{P(x | z)\cdot P(z)}{P(x)}$$ </p> <p>In this case, each variable is: <br> \(P(z)\) is the <strong><em>prior</em></strong> probability of \(z\), which is the initial belief without any knowledge about \(x\). <br> \(P(x)\) is the <strong><em>evidence, or the marginal likelihood</em></strong>, the probability of observing \(x\) across all possible events. <br> \(P(z | x)\) is the <strong><em>posterior</em></strong> probability of \(z\) given \(x\). <br> \(P(x | z)\) is the <strong><em>likelihood</em></strong> of observing \(x\) given \(z\), which assumes the prior is correct.</p> <p>From above, let’s focus on the evidence, or the marginal likelihood. \(P(x)\) can be calculated by: \(P(x) = \displaystyle \int P(x | z)P(z) dz\) since we have a continuous distribution (in VAEs, the latent variable z is assumed to specified to be a Gaussian distribution with a mean of zero and unit variance (\(\mathcal{N}(0, 1)\)). However, this simple-looking integral over the product of gaussian conditional and prior distribution is <strong><em>intractable</em></strong> because the integration is performed over the entire latent space, which is continuous (it is possible to have infinite number of latent variables for a single input).</p> <p>But can we try calculating \(P(x)\) in a different way? We also know that the <em>joint probability</em> \(P(x,z) = P(x)P(z | x)\), meaning that \(P(x) = \frac{P(x,z)}{P(z | x)}\). We quickly realize that this doesn’t work either since we already saw above that the posterior \(P(z | x)\) is unknown! Therefore, we have to resort to approximating the posterior \(P(z | x)\) with an <em>approximate variational distribution \(q_\phi(z | x)\)</em> which has parameters \(\phi\) that needs to be optimized. Hence, in the previous graphical model, the dashed arrow going from x to z represents the variational approximation.</p> <p>But <em>how do we ensure that our approximate variational distribution \(q_\phi(z | x)\) will be as similar as possible to the intractable posterior \(P(z | x)\)?</em> We do this by minimizing the KL-divergence between the two distributions. For two distributions P and Q, KL-divergence essentially measures the difference between the two distributions. The value of the KL-divergence cannot be less than zero, as zero denotes that the two distributions are perfectly equal to each other. Note that \(D_{KL}(P || Q) = \sum_{n=i} P(i) \log \frac{P(i)}{Q(i)}\) <br> Now let’s expand on this:</p> <p> $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(z|x)}{q(z|x)}$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)P(x)} \quad \text{since} \ P(z|x) = \frac {P(x,z)}{P(x)}$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) [\log \frac{P(x,z)}{q(z|x)} - \log P(x)]$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)} + \sum_{n=i} q(z|x) \log P(x)$$ $$min(D_{KL}(q(z|x) || P(z|x))) = - \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)} + \log P(x) \quad \text{since} \ \sum_{n=i} q(z|x) = 1$$ $$\log P(x) = min(D_{KL}(q(z|x) || P(z|x))) + \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)} \quad (1)$$ </p> <p>Observe the left hand side of the last line above (or equation #1). Since we observe \(x\), this is simply a constant. Observe the right hand side now. The first term is what we initially wanted to minimize. The second term is what’s called an <strong><em>“evidence/variational lower bound”, or “ELBO” for short</em></strong>. This is because rather than minimizing our first term (KL-divergence), utilizing the fact that the KL-divergence is always greater than or equal to zero, we can rather maximize the ELBO instead. We know understand why the second term is called a “lower bound”, as \(ELBO \leq \log P(x)\) since \(D_{KL}(q(z|x) || P(z|x)) \geq 0\) all the time. <br> Now let’s expand on ELBO:</p> <p> $$ ELBO = \sum_{n=i} q(z|x) \log \frac{P(x,z)}{q(z|x)}$$ $$ ELBO = \sum_{n=i} q(z|x) \log \frac{P(x|z)P(z)}{q(z|x)}$$ $$ ELBO = \sum_{n=i} q(z|x) [\log P(x|z) + \log \frac{P(z)}{q(z|x)}]$$ $$ ELBO = \sum_{n=i} q(z|x) \log P(x|z) + \sum_{n=i} q(z|x) \log \frac{P(z)}{q(z|x)}$$ $$ ELBO = \sum_{n=i} q(z|x) \log P(x|z) - D_{KL}(q(z|x) || P(z))$$ $$ ELBO = \mathbb{E}_{q(z|x)} [\log P(x|z)] - D_{KL}(q(z|x) || P(z)) \quad (2)$$ </p> <p><strong><em>Remember</em></strong> the last line above (or equation #2) for later, this is also the <strong><em>loss function</em></strong> for training VAEs. But to understand this expression better, let’s now look at VAEs in a <em>neural network’s perspective</em>. A VAE consists of an encoder and a decoder, and both are neural networks. The <em>encoder</em> takes in input data \(x\) and compresses it to latent representation \(z\), and must learn a good latent representation known as the bottleneck of the model. Note that contrary to the encoder of the vanilla autoencoder, the encoder of the variational autoencoder will learn the mean and variance Therefore, the encoder can be denoted as \(q_\phi(z | x)\), where the \(\phi\) is the weights and biases of the model. Note that as previously mentioned, the latent space is assumed to be a Gaussian probability distribution, so sampling from the trained encoder gets us the latent representation \(z\) from data \(x\). The <em>decoder</em> takes in the latent representation <strong>z</strong> from the encoder output and outputs the reconstructed data denoted as \(\hat{x}\), or the parameters to the modeled probability distribution of the data space, and therefore can be denoted as \(p_\theta(x | z)\), where \(\theta\) is also the weights and biases. The below diagram helps us see this entire scheme.</p> <p><img src="/assets/images/autoencoder_diagram.png" width="800" height="420" class="center"></p> <figcaption>Diagram showing autoencoder architecture.</figcaption> <p><br> Now let’s go back to above equation #2. Let’s look at the first term \(\mathbb{E}_{q(z|x)} [\log P(x|z)]\). Now, remember that the latent space \(z\) is assumed to be a Gaussian distribution \(z_i \sim \mathcal{N}(0,1)\). Observe this:</p> <p> $$\log p(x|z) \sim \log exp(-(x-f(z))^2$$ $$\sim |x-f(z)|^2 $$ $$ = |x-\hat{x}|^2 $$ </p> <p>where \(f(z) = \hat{x}\), as the reconstructed image \(\hat{x}\) is the distribution mean \(f(z)\). This is because \(p(x|z) \sim \mathcal{N}(f(z), I)\). Therefore, here we see that the first term is correlated to the mean squared error (MSE) loss between the original image and the reconstructed image. This makes sense, as during training, we want to make penalize the model if the reconstructed image is too dissimilar to the original image. It is important to see that this was the first term of the <em>ELBO</em>, and remember we want to maximize this. Maximizing the first term is then therefore correlated to minimizing the MSE/reconstruction loss.</p> <p>Let’s now look at the second term, \(-D_{KL}(q(z|x) || P(z))\) (note the negative sign) which is the KL-divergence between our learned gaussian distribution (encoder) \(q(z|x)\) and the prior (latent space) gaussian distribution. Remember this is the second term of ELBO, so we still want to maximize- but note the negative sign, we actually want to minimize the KL divergence between the two- which makes sense as we want to encourage the learned distribution from the encoder to be similar to the unit Gaussian prior.</p> <p><img src="/assets/images/VAE_problem.png" width="800" height="400" class="center"></p> <figcaption>Diagram showing VAE latent space with KL-regularization (left) and without KL-regularization (right).</figcaption> <p><br></p> <p>This actually ties back to the KL-regularization of LDMs in the previous blog (part 2), which is the diagram showing the VAE latent space with and without KL-regularization. This is re-shown above. The minimization of KL divergence shown above regularizes the latent space as the “clusters” itself are bigger and are more centered around within each other. This ensures that the decoder creates <i>diverse and accurate samples</i>, as there would be smoother transitions between different classes (clusters). This is why both reconstruction loss term and KL-divergence term are included in the VAE loss function during training.</p> <p><img src="/assets/images/mnist_latent_space.jpg" width="600" height="600" class="center"></p> <figcaption>Diagram showing regularized VAE latent space of MNIST dataset.</figcaption> <p><br> For example, as seen above for MNIST handwritten digits, we see that the classes, or clusters have a smooth transition in this latent space. Without regularization, the encoder can cheat by learning narrow distributions with low variances. Now that we’ve understood the importance of maximizing ELBO to train a VAE, let’s go back to LDMs.</p> <hr> <p><a id="model-objective-1"></a></p> <h3 id="model-objective-1"><strong><em>Model Objective:</em></strong></h3> <p>Now why did we go over the VAEs and its variational approximation process? This is because diffusion models have a very similar set up to VAEs in that it also has a tractable likelihood that can be maximized by maximizing the ELBO to ensure that the approximate posterior is as similar as possible to the unknown true posterior we’d like to model. We’re going to derive the training loss function, or the <em>model objective</em> just like how it was done for VAEs.</p> <p>Let’s first look at the forward and the backward diffusion process in a probabilistic way, since we already know about the diffusion processes in neural networks (train a regularized autoencoder for forward and backward diffusion process!). Take a look at the graphical model below:</p> <p><img src="/assets/images/forward_backward_diffusion.png" width="929" height="181" class="center"></p> <figcaption>Graphical model showing the forward and reverse diffusion process.</figcaption> <p><br> The forward diffusion process actually is the reverse of the above diagram, as the arrows should be going the opposite way- the forward diffusion process adds noise to a specific data point \(x_0\) that is sampled from the unknown, true distribution we’d like to model. Then, \(x_0\) has Gaussian noise added to it in a Markovian process (from \(x_{t-1}\) all the way to \(x_T\)) with \(T\) steps. Therefore, \(q(x_t|x_{t-1})\) takes the image and outputs a slightly more noisy version of the image. This can be formulated below:</p> <p> $$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \mu_t = \sqrt{1-\beta_t}x_{t-1},\Sigma_t = \beta_tI) \quad (3)$$ </p> <p>Assuming high-dimensionality, \(q(x_t|x_{t-1})\) is a Gaussian distribution with the above defined mean and variance. Note that for each dimension, it has the same variance \(\beta_t\). \(\beta_t\) is a number between 0 and 1, and essentially scales the data so the variance doesn’t grow out of proportion. The authors use a <em>linear schedule</em> for \(\beta_t\), meaning that \(\beta_t\) is linearly increased as the image gets noised more. Note that with above formula, we can easily obtain desired noised image at timestep \(T\) by using the Markovian nature of the process. Below is a tractable, closed-form formula to sample a noised image at any timestep:</p> <p> $$q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1}) \quad (4)$$ </p> <p>Basically, if T = 200 timesteps, we would have 200 products to sample the noised image \(x_{t=200}\). However, if the timestep gets larger, we run in to trouble of computational issues. Therefore, we utilize the <em>reparametrization trick</em> which gives us a much simpler tractable, closed-form formula for sampling that requires much fewer computations.</p> <p>The reparametrization trick is used whenever we sample from a distribution (Gaussian in our case) that is not directly differentiable. For our case, the mean and the variance of the distribution are both dependent on the model parameters, which is learned through SGD (as shown above). The issue is that because sampling from the Gaussian distribution is stochastic, we cannot compute the gradient anymore to update the mean and variance parameters. So, we introduce the auxiliary random variable \(\epsilon\) that is deterministic since it is sampled from a fixed standard Gaussian distribution (\(\epsilon \sim \mathcal{N}(0, 1)\)), which allows SGD to be possible since \(\epsilon\) is not dependent on the model parameters. Therefore, the reparametrization trick \(x = \mu + \sigma * \epsilon\) works by initially computing the mean and standard deviation using current weights given input data, then drawing deterministic random variable \(\epsilon\) to obtain the desired sample \(x\). Then, loss can be computed with respect to mean and variance, and they can be backpropagated via SGD.</p> <p> $$ \text{Let} \quad \alpha_t = 1 - \beta_t \text{and} \quad \hat{\alpha}_t = \prod_{i=1}^{t} \alpha_i $$ $$ \text{Also sample noise} \quad \epsilon_0, ..., \epsilon_{t-1} \sim \mathcal{N}(0,I) $$ $$ \text{Then,} \quad x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon_{t-1} $$ $$ x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1} $$ $$ x_t = \sqrt{\alpha_t \alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_t \alpha_{t-1}}\epsilon_{t-2} $$ $$ x_t = \quad ... $$ $$ x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}{\epsilon}_0 $$ $$ \mathbf{ \text{Therefore, since } \quad \mu + \sigma * \epsilon, \quad q(x_t|x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha}_t}x_0,\Sigma_t = (1-\hat{\alpha}_t)I)} \quad (5)$$ </p> <p><em>Note that above simplification is possible since the variance of two merged Gaussians is simply the sum of the two variances.</em></p> <p>To summarize the forward diffusion process, we can think of this as the encoder (remember encoder performs forward diffusion process to map pixel space to latent space) Each time step or each encoder transition is denoted as \(q(x_t|x_{t-1})\) which is from a fixed parameter \(\mathcal{N}(x_t,\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)\). Note that like VAE encoder, the encoder distribution for the forward diffusion process is also modeled as multivariate Gaussian. However, in VAEs, we learn the mean and variance parameters, while forward diffusion has fixed specific means and variances at each timestep as seen in equation #4.</p> <p>Now, look at the graphical model again for the reverse diffusion process, which is denoted by \(p_\theta(x_{t-1}|x_t)\). Now, if we could reverse the above forward diffusion process \(q(x_t|x_{t-1})\) and sample from \(q(x_{t-1}|x_t)\), we can easily run inference by sampling from our Gaussian noise input which is \(\sim \mathcal{N}(0,I)\). However, <em>this is exactly the same problem we had for VAEs as above !</em> This true posterior is unknown and is intractable since we have to compute the entire data distribution or marginal likelihood/evidence, \(q(x)\). Here, we can treat \(x_0\) as the true data, and every subsequent node in the Markovian chain \(x_1,x_2...x_T\) as a latent variable. Therefore, we approach this problem the exact same way.</p> <p>We approximate the true posterior \(q(x_{t-1}|x_t)\) with a neural network or a “decoder” that has parameters \(\theta\) (Note that this denoising diffusion “decoder” is the UNet, please don’t confuse this to the decoder of the autoencoder, which is something completely different as it is responsible for bringing the final output of \(x_0\) back to the pixel space. As previously discussed, we utilize UNet because of its inductive bias to images and its compatibility with cross attention). Like the forward process, but just reversing the timestep, we have:</p> <p> $$ p_{\theta}(x_{0:T}) = p(x_T) \prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) \quad (6)$$ </p> <p>This is just like the forward process in equation #4 but in reverse. By applying this formula, we can go from pure noise \(x_T\) to the approximated data distribution. Remember the encoder does not have learnable parameters (pre-defined or fixed), so we only need to train the decoder in learning the conditionals \(p_{\theta}(x_{t-1}|x_t)\) so we can generate new data. Conditioning on the previous timestep \(t\) and previous latent \(x_t\) lets the decoder learn the Gaussian parameters \(\theta\) which is the mean and variance \(\mu_{\theta},\Sigma_{\theta}\) (Since we assume variance is fixed due to noise schedule \(\beta\), however, we just need to learn the mean with the decoder). Therefore, running inference on a LDM only requires the decoder as we sample from pure Gaussian noise \(p(x_T)\) and run T timesteps of the decoder transition \(p_{\theta}(x_{t-1}|x_t)\) to generate new data sample \(x_0\). If our approximated \(p_{\theta}(x_{t-1}|x_t)\) steps are similar to unknown, true posterior steps \(q(x_{t-1}|x_t)\), the generated sample \(x_0\) will be similar to the one sampled from the training data distribution.</p> <p><em>Therefore, we want to train our decoder to find the reverse Markov transitions that will maximize the likelihood of the training data. Now, how do we train this denoising/reverse diffusion model?</em> We utilize the <strong><em>ELBO</em></strong> again. Remember for equation #1, we saw that the VAE was optimized by maximizing the ELBO (which was essentially the same as minimizing the negative log likelihood), we do the same below. Like VAEs, we first want to minimize the KL-divergence between the true unknown posterior \(q(x_{1:T}|x_0)\) and the approximated posterior \(p_{\theta}(x_{1:T}|x_0)\):</p> <p> $$ 0 \leq min \ D_{KL}(q(x_{1:T}|x_0)||p_{\theta}(x_{1:T}|x_0)) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log (p_{\theta}(x_0)) + min \ D_{KL}(q(x_{1:T}|x_0)||p_{\theta}(x_{1:T}|x_0)) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log (p_{\theta}(x_0)) + \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)})$$ $$ - \log (p_{\theta}(x_0)) \leq - \log (p_{\theta}(x_0)) + \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}) + \log(p_{\theta}(x_0)) \quad \text{since} \ p_{\theta}(x_{1:T}|x_0) = \frac{p_{\theta}(x_0|x_{1:T})p_{\theta}(x_{1:T})}{p_{\theta}(x_0)} = \frac{p_{\theta}(x_0,x_{1:T})}{p_{\theta}(x_0)} = \frac{p_{\theta}(x_{0:T})}{p_{\theta}(x_0)}$$ $$ - \log (p_{\theta}(x_0)) \leq \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}) \quad \text{since} \ - \log (p_{\theta}(x_0)) + \log(p_{\theta}(x_0)) = 0 \quad (7)$$ </p> <p>As seen in above, minimizing the KL-divergence also gives us the form \(- \log P(x) \leq ELBO\) or \(-ELBO \leq \log P(x)\), as we saw for VAEs in equation #1, since the RHS of equation #7 above is the <strong><em>ELBO</em></strong> for LDMs (\(ELBO_{LDM} = \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T}})\)). Therefore, instead of minimizing the above KL-divergence, we can maximize the ELBO like VAEs:</p> <p> $$ - \log (p_{\theta}(x_0)) \leq \log(\frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}) $$ $$ - \log (p_{\theta}(x_0)) \leq \log(\frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{p(x_T) \prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_t)}) \quad \text{since} \ p_{\theta}(x_{0:T}) = p(x_T) \prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t)$$ $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \log (\frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_t)}) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=1}^{T} \log(\frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}) $$ $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}) + \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \quad \text{since} \ \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \ \text{is when} \ t = 1 \quad (8)$$ </p> <p>Note equation #8 above, and focus on \(q(x_t|x_{t-1})\) term. This is essentially the reverse diffusion step, but it is only conditioned on the pure Gaussian noise. The latent image vector \(x_{t-1}\) thus has a high variance, but by also conditioning on original image \(x_0\), we can decrease the variance and therefore enhance the image generation quality (think about it, if model is conditioned only on pure Gaussian noise, the produced latent image would vary more than the model conditioned on pure Gaussian noise <em>and</em> the original image as well). This is achieved by using the Baye’s rule:</p> <p> $$q(x_t|x_{t-1}) = \frac{q(x_{t-1}|x_t)q(x_t)}{q(x_{t-1})} = \frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}$$ </p> <p>Substituting this to equation #8 gives equation #9:</p> <p> $$ - \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{p_{\theta}(x_{t-1}|x_t)q(x_{t-1}|x_0)}) + \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \quad (9)$$ </p> <p>For equation #9, we can further split the second term on the RHS (the summation term) to two different summation terms to further simplify the RHS:</p> <p> $$ \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{p_{\theta}(x_{t-1}|x_t)q(x_{t-1}|x_0)}) = \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \sum_{t=2}^{T} \log(\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)})$$ </p> <p>Examining \(\sum_{t=2}^{T} \log(\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)})\), for any \(t&gt;2\), we see that all the terms in the denominator and numerator will cancel out each other and will simplify to \(\log(\frac{q(x_t|x_0)}{q(x_1|x_0)})\). <br> Performing all of these substitutions to equation #9 gives equation #10:</p> <p> $$- \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \log(\frac{q(x_t|x_0)}{q(x_1|x_0)}) + \log(\frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}) \quad (10)$$ </p> <p>Now take the last two terms of the RHS in equation #10 above and further simplify by expanding the log:</p> <p> $$- \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \log(q(x_t|x_0)) - \log(q(x_1|x_0)) + \log(q(x_1|x_0)) - \log(p_{\theta}(x_0|x_1))$$ $$- \log (p_{\theta}(x_0)) \leq - \log(p(x_T)) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) + \log(q(x_t|x_0)) - \log(p_{\theta}(x_0|x_1))$$ $$- \log (p_{\theta}(x_0)) \leq \log(\frac{q(x_t|x_0)}{p(x_T)}) + \sum_{t=2}^{T} \log(\frac{q(x_{t-1}|x_t,x_0)}{p_{\theta}(x_{t-1}|x_t)}) - \log(p_{\theta}(x_0|x_1))$$ $$- \log (p_{\theta}(x_0)) \leq D_{KL}(q(x_T|x_0)||p(x_T)) + \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t)) - \log(p_{\theta}(x_0|x_1)) \quad (11)$$ </p> <p>Now look at equation #11 above, which is simplified further thanks to the definition of KL-divergence. The first RHS term \(D_{KL}(q(x_T|x_0)||p(x_T))\) has no learnable parameters, as we previously talked about the encoder \(q(x_T|x_0)\) having no learnable parameters as the forward diffusion process is fixed by the noising schedule shown in equation #3 and #5. Additionally, \(p(x_T)\) is just pure Gaussian noise as well. Lastly, it is safe to assume that this term will be zero, as q will resemble p’s random Gaussian noise and bring the KL-divergence to zero. Therefore, below is our final training objective, all we need to do is minimize the RHS of the equation:</p> <p> $$ - \log (p_{\theta}(x_0)) \leq \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t)) - \log(p_{\theta}(x_0|x_1)) \quad (12)$$ </p> <p>Now, to minimize the RHS of the equation our only choice is to minimize the first term \(\sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))\). Before diving into the derivation, let’s look at what this term actually means- it is the KL divergence between the ground truth denoising transition step \(q(x_{t-1}|x_t,x_0)\) and our approximation of the denoising transition step \(p_{\theta}(x_{t-1}|x_t)\), and it makes sense we want to minimize this KL divergence since we want the approximated denoising transition step to be as similar to the ground truth denoising transition step as possible.</p> <p>Utilizing Baye’s Rule, we can calculate the desired ground truth denoising step \(q(x_{t-1}|x_t,x_0)\) :</p> <p> $$ q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \quad (13) $$ </p> <p>Now, we know the form of the distribution of the denominator of equation #13 above, which is \(q(x_t|x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha_t}}x_0,\Sigma_t = (1-\hat{\alpha_t})I)\) Recall that this is from equation #5 from above and this was the reparametrization trick for the simplification of the forward diffusion process, or \(q(x_t|x_0)\) : \(x_t = \sqrt{\hat{\alpha}_t}x_0 + \sqrt{1-\hat{\alpha}_t}\epsilon\).</p> <p>Now, how about the numerator? We also know the forms of the two distributions in the numerator of equation #1 above as well. \(q(x_t \mid x_{t-1},x_0)\)is the forward diffusion noising step and is formulated in equation #3 above \(q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \mu_t = \sqrt{1-\beta_t}x_{t-1},\Sigma_t = \beta_tI) = q(x_t \mid x_{t-1}, x_0) = \mathcal{N}(x_t; \mu_t = \sqrt{\alpha_t}x_{t-1},\Sigma_t = (1-\alpha_t)I)\) where \(\alpha_t = 1-\beta_t\). The other distribution \(q(x_{t-1} \mid x_0)\) is a slight modification of the distribution in the numerator \(q(x_t|x_0)\), with \(t\) being \(t-1\) instead, so this is formulated as:</p> <p> $$q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \mu_t = \sqrt{\hat{\alpha}_{t-1}}x_0,\Sigma_t = (1-\hat{\alpha}_{t-1})I)$$ </p> <p>Now inputting all three of these formulations in the Baye’s Rule above in equation #13 we get equation #14 below:</p> <p> $$q(x_{t-1} \mid x_t,x_0) = \frac{\mathcal{N}(x_t; \mu_t = \sqrt{\alpha_t}x_{t-1},\Sigma_t = (1-\alpha_t)I) \mathcal{N}(x_{t-1}; \mu_t = \sqrt{\hat{\alpha}_{t-1}}x_0,\Sigma_t = (1-\hat{\alpha}_{t-1})I)}{\mathcal{N}(x_t; \mu_t = \sqrt{\hat{\alpha}_t}x_0,\Sigma_t = (1-\hat{\alpha}_t)I)} \quad (14)$$ </p> <p>Now, combining the three different Gaussian distributions above to get the mean and variance for the desired \(q(x_{t-1} \mid x_t,x_0)\) is a lot of computations to show in this blog. The full derivation, for those who are curious, can be found in this <a href="https://arxiv.org/pdf/2208.11970.pdf" rel="external nofollow noopener" target="_blank">link</a>, <em>exactly in page 12 from equation 71 to 84</em>. (I just feel like this derivation is just a bunch of reshuffling variables with algebra, so it is unnecessary to include in my blog) Finishing this derivation shows that our desired \(q(x_{t-1} \mid x_t,x_0)\) is also normally distributed with the below formulation:</p> <p> $$q(x_{t-1} \mid x_t,x_0) \sim \mathcal{N}(x_{t-1}; \mu_t = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}},\Sigma_t = \frac{(1-\alpha_t)(1-\hat{\alpha}_{t-1})}{(1-\hat{\alpha_t})}I \quad (15)$$ </p> <p>From above, we can see that the above approximate denoising transition \(q(x_{t-1} \mid x_t,x_0)\) has mean that is a function of \(x_t\) and \(x_0\) and therefore can be abbreviated as \(\mu_q(x_t,x_0)\), and has variance that is a function of \(t\) (naturally) and the \(\alpha\) coefficients and therefore can be abbreviated as \(\Sigma_q(t)\). Recall that these \(\alpha\) coefficients are fixed and known, so that at any time step \(t\), we know the variance.</p> <p>Now, back to equation #12 where we want to minimize the KL-divergence:</p> <p> $$ \mathop{\arg \min}\limits_{\theta} \quad D_{KL}(q(x_{t-1} \mid x_t,x_0)||p_{\theta}(x_{t-1} \mid x_t)) $$ </p> <p>Equation #15 above tells us the formulation for ground truth denoising transition step \(q(x_{t-1} \mid x_t,x_0)\) , and we know the formulation for our approximate denoising transition step \(p_{\theta}(x_{t-1} \mid x_t)\).</p> <p>What is the KL-divergence between two Gaussian distributions? It is:</p> <p> $$ D_{KL}(\mathcal{N}(x;\mu_x,\Sigma_x) || \mathcal{N}(y;\mu_y,\Sigma_y)) = \frac{1}{2} [ \log \frac{\Sigma_y}{\Sigma_x} - d + tr({\Sigma_y}^{-1}\Sigma_x) + (\mu_y - \mu_x) ^ {T} {\Sigma_y}^{-1} (\mu_y - \mu_x) ] $$ </p> <p>Applying this KL-divergence equation to equation #12 above is also just reshuffling algebra, which is shown in the same link as before, from equations 87 to 92. We can see that equation #12 is simplified to:</p> <p> $$ \mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{|| \mu_{\theta} - \mu_q ||}^{2}] \quad (16) $$ </p> <p>To explain equation #16 above, \(\mu_q\) is the mean of the ground truth denoising transition step \(q(x_{t-1} \mid x_t,x_0)\) and \(\mu_{\theta}\) is the mean of our desired approximate denoising transition step \(p_{\theta}(x_{t-1} \mid x_t)\). How do we get these two values? We calculated them at equation 15, we can just utilize the \(\mu_t\), it depends on \(x_0\) and \(x_t\)! But wait, while \(q(x_{t-1} \mid x_t,x_0)\) is dependent on \(x_0\) and \(x_t\), \(p_{\theta}(x_{t-1} \mid x_t)\) is only dependent on \(x_t\), but not \(x_0\)! Well this is exactly what we’re trying to do, our approximate denoising step \(\hat{x}_{\theta}(x_t,t)\) is parametrized by the neural network with \(\theta\) parameters, we predict the generated/original image \(x_0\) using noisy image \(x_t\) and time step \(t\)!</p> <p>We see why it’s important to do derivations, it exactly shows what the objective is here now: train a neural network that parametrizes \(\hat{x}_{\theta}(x_t,t)\) to predict \(x_0\) as accurately as possible to make our approximate denoising step as similar to the ground truth denoising step as possible! \(\mu_q\) and \(\mu_{\theta}\), using equation #15 is:</p> <p> $$\mu_q = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}}$$ $$\mu_{\theta} = \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_{\theta}(x_t,t)}{1-\hat{\alpha_t}}$$ </p> <p>Note that the two are exactly the same except \(x_0\) is replaced with \(\hat{x}_{\theta}(x_t,t)\) as mentioned before. Finally, plugging these two into equation #16 allows us to find the training objective:</p> <p> $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \mu_{\theta} - \mu_q ||}^{2}] \quad $$ $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_{\theta}(x_t,t)}{1-\hat{\alpha_t}} - \frac{\sqrt{\alpha_t}(1-\hat{\alpha}_{t-1})x_t + \sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}} ||}^{2}]$$ The first term of each term is the same, so after eliminating those: $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \frac{\sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)\hat{x}_{\theta}(x_t,t)}{1-\hat{\alpha_t}} - \frac{\sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\hat{\alpha_t}} ||}^{2}]$$ $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} [{ || \frac{\sqrt{\hat{\alpha}_{t-1}}(1-\alpha_t)}{1-\hat{\alpha_t}}(\hat{x}_{\theta}(x_t,t)-x_0)||}^{2}]$$ $$\mathop{\arg \min}\limits_{\theta} \quad \frac{1}{2{\sigma_q}^{2}(t)} \frac{\hat{\alpha}_{t-1}(1-\alpha_t)^{2}}{(1-\hat{\alpha_t)^{2}}} [{||(\hat{x}_{\theta}(x_t,t)-x_0)||}^{2}] \quad (17)$$ </p> <p>Equation #17 is finally our training objective. <em>To summarize again, we are learning the parameters \({\theta}\) from training a neural network to predict the ground truth image \(x_0\) from noised version of the image \(x_t\).</em> What’s important to take away from this, however, is understanding that, ultimately, applying the same ELBO method to maximize ELBO led to minimizing the KL divergence between ground truth and approximate denoising transition step, and this happens to be a form of <strong><em>minimizing the mean-squared-error (MSE) between the two distributions</em></strong> as seen in equation #17 above. This is quite <em>fascinating</em>, as all of this derivation just boils down to a simple MSE-like loss function.</p> <p>Now, with the training objective derived, the training algorithms and sampling algorithms will be explained in the next two parts of this blog with more mathematical details on LDMs that were not covered yet, especially regarding training/inference algorithms and conditioning/classifier-free guidance.</p> <hr> <p><em>Image credits to:</em></p> <ul> <li><a href="https://arxiv.org/pdf/1312.6114.pdf" rel="external nofollow noopener" target="_blank">VAE Directed Graphical Model</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/cvae" rel="external nofollow noopener" target="_blank">MNIST Latent Space Example</a></li> <li><a href="https://arxiv.org/pdf/2006.11239.pdf" rel="external nofollow noopener" target="_blank">Graphical Model of Diffusion</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Kevin (Won June) Cho. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>