<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>(Concept Review/Segmentation Models) How to use Class Activation Maps (CAM) for Explainable AI in Semantic Segmentation | Kevin (Won June) Cho</title> <meta name="author" content="Kevin (Won June) Cho"> <meta name="description" content="Introduction to CAM, explanation of GradCAM and HiResCAM in semantic segmentation, showcasing some example code for CAM and CAM image results in my project!"> <meta name="keywords" content="deep-learning, image-segmentation, python, pytorch"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://chokevin8.github.io/blog/2023/class-activation-maps/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Kevin (Won June) Cho</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">My Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">My Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">My Github</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">My Resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">(Concept Review/Segmentation Models) How to use Class Activation Maps (CAM) for Explainable AI in Semantic Segmentation</h1> <p class="post-meta">October 20, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/concept-review"> <i class="fas fa-hashtag fa-sm"></i> concept-review</a>   <a href="/blog/tag/segmentation-model"> <i class="fas fa-hashtag fa-sm"></i> segmentation-model</a>     ·   <a href="/blog/category/posts"> <i class="fas fa-tag fa-sm"></i> posts</a>   </p> </header> <article class="post-content"> <p>In this post, I will briefly describe Class Activation Maps (CAM) and some of its popular subtypes, usages in semantic segmentation, and then finally post some code and results in utilizing CAM in my own semantic segmentation project using <a href="/projects/1_project/">H&amp;E images</a>.</p> <hr> <h2 id="table-of-contents"><strong>Table of Contents:</strong></h2> <ul> <li> <h3 id="what-are-class-activation-maps-cam"><a href="#what-are-cam">What are Class Activation Maps (CAM)?</a></h3> </li> <li> <h3 id="cams-in-image-semantic-segmentation"><a href="#cams-in-image-seg">CAMs in Image Semantic Segmentation:</a></h3> </li> <li> <h3 id="utilizing-cam-in-my-project"><a href="#cam-in-my-proj">Utilizing CAM in My Project</a></h3> </li> </ul> <hr> <p><a id="what-are-cam"></a></p> <h2 id="what-are-class-activation-maps-cam-"><strong>What are Class Activation Maps (CAM) ?</strong></h2> <p><em>Class activation maps</em>, henceforth referred to as <em>CAM</em>, can be thought of as heatmaps that can highlight the regions of a query image that are most important for the network’s classification or segmentation decision. Before introducing CAMs, <em>why do we want to look at CAMs in the first place?</em></p> <p>CAMs are probably one of the most popular and easy-to-do explainable AI metrics, which aim to understand and interpret the decisions made by built machine learning models. Let’s come up with an example where CAMs would be useful: Let’s say I work for a AI company and we built a machine learning model that looks at a chest x-ray and is able to detect a rare disease that occurs 1% of the time with 99% sensitivity (so the model itself is sensitive (or has high recall), so it isn’t saying no all the time and has 99% accuracy). However, by utilizing CAMs before deploying the model, I find out that the model is actually only looking at a circle at the corner of the x-ray image to detect the rare disease. Upon checking the images used for training/validation/testing, they all had a circle at the corner that the hospital used to specify that this was a positive image. <strong>Without CAMs, we would have no idea this was the case and would’ve been deploying a completely useless model!</strong></p> <p>Like the above made-up example, traditionally, CAMs were developed to be used for image classification, as utilization of CAM was limited to specific types of architectures, which are CNNs with global average pooling (GAP) and a final fully connected layer. Most image classification models use GAP and a final fully connected layer followed by the output activation function (assume multi-class, so we use softmax) for turning logits into final prediction probability/results. The last layer before the GAP and fully connected layer is the layer that holds the “feature map”, which captures subtle, fine semantic details of the training images. The implementation of CAM is beautifully summarized in the diagram below:</p> <p><img src="/assets/images/CAMs/CAM_summarized.jpg" width="835" height="392" class="center"></p> <figcaption> Diagram showing CAM for an image classification task. </figcaption> <p><br></p> <p>As seen in the above diagram, the class activation map is generated by a linear combination of all the \(n\) weights (weights in the fully connected layer) for the specific class (in the above case, the Australian terrier) and all the \(n\) feature maps. For example, in the diagram above, we can tell that \(w_2\) would have a higher weight than \(w_1\) since the feature map \(F_2\) has to do with the Australian terrier. Mathematically, the CAM can be represented as:</p> <p> $$Y_c = \sum_{k} {w_k}^{c} \cdot \frac{1}{Z} \sum_{i}\sum_{j} A_{(i,j)}^{k}$$ $$Y_c = \sum_{k} {w_k}^{c} \cdot F^{k} \quad (1)$$ </p> <p>,where \(Y_c\) is the activation score (CAM) for \(c\), which is the specific class (like Australian terrier). \(k\) is the number of feature maps and therefore \({w_k}^{c}\) is the weight for \(k\)th feature map for class \(c\). Lastly, \(A_{(i,j)}^{k}\) is the \(k\)th feature map for pixel coordinate \((i,j)\), which is summed over all \(i\) and \(j\) and divided by total number of pixels \(Z\) to return our global average pooled feature map \(F^{k}\). We can see that \(\frac{1}{Z} \sum_{i}\sum_{j}\) is the mathematical representation of global average pooling (GAP). Finally, since feature maps are downsampled compared to the original image size, we need to perform bi-linear interpolation on the CAM to upsample for us to visualize the overlay of the CAM on the query image like shown in the diagram above.</p> <p>However, as mentioned above, the general CAM method requires an architecture that includes all of the three:</p> <ol> <li>A feature map, or the penultimate layer of the model.</li> <li>A global average pooling operation to the feature map.</li> <li>A final fully connected layer followed by an activation function to produce prediction labels.</li> </ol> <p>Since not all methods satisfy all three requirements above, CAM was only limited to certain types of architecture, making it unavailable to use for other types of CNN architectures that are dense like VGG, handle multi-modal inputs/perform reinforcement learning.</p> <p><a id="cams-in-image-seg"></a></p> <h2 id="cams-in-image-semantic-segmentation-1"><strong>CAMs in Image Semantic Segmentation:</strong></h2> <p>As mentioned previously, the three requirements for CAM severely limited the usage to certain types of architectures, and was totally not applicable for non-CNN based models such as vision transformers, or ViTs (to be fair, when CAM was first released, transformers were not a thing yet). Therefore, GradCAM, or Gradient-weighted CAMs, were widely used which essentially replaces the weights of the fully connected layer with calculated gradients that flow back into the last convolutional layer. To show that this is true, consider taking the gradient of the activation score for class \(c\) (\(Y^{c}\)) with respect to feature map \(F^{k}\) from the above CAM equation, or equation #1:</p> <p> $$ Y_c = \sum_{k} {w_k}^{c} \cdot F^{k}$$ $$\frac{\delta Y^{c}}{\delta F^{k}} = \frac{\frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}} {\frac{\delta F^{k}}{\delta A_{(i,j)}^{k}}} $$ $$\text{Recall from above: } F^{k} = \frac{1}{Z} \sum_{i}\sum_{j} \text{ so: } \frac{\delta F^{k}}{\delta A_{(i,j)}^{k}} = \frac{1}{Z}$$ $$\text{Then: } \frac{\delta Y^{c}}{\delta F^{k}} = \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \cdot Z $$ $$\text{Recall from above: } Y_c = \sum_{k} {w_k}^{c} \cdot F^{k} \text{ so: } \frac{\delta Y^{c}}{\delta F^{k}} = {w_k}^{c}$$ $$\text{Then: } {w_k}^{c} = Z \cdot \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (2)$$ </p> <p>Now note that above equation #2 is only for pixel location \((i,j)\), so let’s sum over all pixels. Note that \(\sum_{i}\sum_{j} 1 = Z\):</p> <p> $$\sum_{i}\sum_{j} {w_k}^{c} = Z \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}$$ $$Z{w_k}^{c} = Z \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}$$ $${w_k}^{c} = \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (3)$$ </p> <p>The equation #3 is an important result, this shows us that the weights for feature map \(k\) for class \(c\) are equal to the gradient of the activation score with respect to the \(k\)th feature map. But remember, we’re no longer using the weights of the fully connected layer! By re-introducing the normalization constant \(1/Z\) for global average pooling, and pooling over the gradients instead, we obtain neural importance weights \(\alpha_k^{c}\) instead:</p> <p> $${\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (4)$$ </p> <p>We can think of this neural importance weight \({\alpha_k}^{c}\) as the equivalent to the \({w_k}^{c}\) in vanilla CAMs. Therefore, use equation #1 for CAM above and sum over all \(k\) feature maps to find our equation for evaluating our activation score for Grad-CAM:</p> <p> $$Y_c = \sum_{k} {\alpha_k}^{c} \cdot F^{k} \text{ where } {\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} (5)$$ </p> <p>Equation #5 is one-step short of completion. Why? The gradients can be negative or positive! We are only interested in finding features that have a positive influence on class \(c\). This means that the activation score should be positive, and to have a positive gradient, the feature map should be positive as well. Therefore, with Grad-CAM we can highlight regions in the image, or features, that contribute to increasing the class activation score. On the other hand, negative gradients most likely belong to features in the image that are correlated with classes that are not \(c\), and therefore we apply ReLU function to suppress negative gradients and only keep positive gradients:</p> <p> $$Y_c = ReLU (\sum_{k} {\alpha_k}^{c} \cdot F^{k}) \text{ where } {\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} (6)$$ </p> <p>Equation #6 above is the final equation for Grad-CAM. Now, even though CAMs were widely used for classification, they could definitely also be used for semantic segmentation tasks where each pixel is labeled as a class. In this case, however, we have to modify equation #6 a bit. This is because image classification outputs a single class distribution (ex. this image is a dog), image semantic segmentation doesn’t, as it outputs logits for every pixel \((a,b)\) predicted for class \(c\). Therefore, it makes sense to sum all of these pixels as the activation score so that it becomes a single class distribution like image classification. We therefore modify the \(Y^{c}\) in the gradient to \(\sum_{(a,b) \in M}{Y_{(a,b)}}^{c}\) where \(M\) is a set of all pixel indices that belong to class \(c\) in the segmentation prediction. The final equation for Grad-CAM in image segmentation is shown below:</p> <p> $$Y_c = ReLU (\sum_{k} {\alpha_k}^{c} \cdot F^{k}) \text{ where } {\alpha_k}^{c} = \frac{1}{Z} \sum_{i}\sum_{j} \frac{\delta \sum_{(a,b) \in M}{Y_{(a,b)}}^{c}}{\delta A_{(i,j)}^{k}} (7)$$ </p> <p>However, if we look at the equation #6 or #7 above for Grad-CAM carefully, there is a critical issue- when calculating the neural importance weight \({\alpha_k}^{c}\) the gradients are averaged due to global average pooling (GAP). Why can this be a problem? Look at the diagram below: <img src="/assets/images/hirescam.png" width="744" height="480" class="center"></p> <figcaption> Diagram comparing HiResCAM and GradCAM and how HiResCAM alleviates the problem of averaging the gradients in GradCAM. </figcaption> <p><br> As seen in the diagram above, we see an example of a 3 x 3 feature map. Note the positive/negative pattern of this feature map. With HiResCAM, the feature map is multiplied by the gradients in an element-wise matter, and we can see that the positive and negative gradients are taken into account in the resulting HiResCAM. However, with Grad-CAM the gradients are all averaged out, and therefore the negative gradients are actually suppressed, and therefore the resulting Grad-CAM retains its original positive/negative feature map pattern. Therefore, we see that HiResCAM produces accurate attention.</p> <p><a id="cam-in-my-proj"></a></p> <h2 id="utilizing-cam-in-my-project-1"><strong>Utilizing CAM in My Project:</strong></h2> <p>Now that we know the differences between GradCAM and HiResCAM, below is the code that I utilized to generate CAM for my skin H&amp;E tissue images to assess how my DeepLabv3+ image segmentation segments the classes and if it is cheating or not!</p> <p>First import relevant packages, including our pytorch-grad-cam library from the official <a href="https://github.com/jacobgil/pytorch-grad-cam" rel="external nofollow noopener" target="_blank">repo</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">requests</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">natsort</span> <span class="kn">import</span> <span class="n">natsorted</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam.utils.image</span> <span class="kn">import</span> <span class="n">show_cam_on_image</span><span class="p">,</span> <span class="n">preprocess_image</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam</span> <span class="kn">import</span> <span class="n">GradCAM</span><span class="p">,</span> <span class="n">HiResCAM</span><span class="p">,</span> <span class="n">GradCAMElementWise</span><span class="p">,</span> <span class="n">ScoreCAM</span><span class="p">,</span> <span class="n">GradCAMPlusPlus</span><span class="p">,</span> <span class="n">AblationCAM</span><span class="p">,</span> <span class="n">XGradCAM</span><span class="p">,</span> <span class="n">EigenCAM</span><span class="p">,</span> <span class="n">EigenGradCAM</span><span class="p">,</span> <span class="n">FullGrad</span><span class="p">,</span> <span class="n">LayerCAM</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam.utils.model_targets</span> <span class="kn">import</span> <span class="n">ClassifierOutputTarget</span>
<span class="kn">from</span> <span class="n">pytorch_grad_cam.utils.image</span> <span class="kn">import</span> <span class="n">show_cam_on_image</span>
<span class="n">Image</span><span class="p">.</span><span class="n">MAX_IMAGE_PIXELS</span> <span class="o">=</span> <span class="bp">None</span>
<span class="kn">import</span> <span class="n">segmentation_models_pytorch</span> <span class="k">as</span> <span class="n">smp</span>
<span class="kn">import</span> <span class="n">albumentations</span> <span class="k">as</span> <span class="n">A</span>
<span class="kn">from</span> <span class="n">albumentations.pytorch</span> <span class="kn">import</span> <span class="n">ToTensorV2</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Then, write your own function to load relevant model (for me, my DeepLabV3+ segmentation model) and if necessary, your own dataloading function as well:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_transforms</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="nc">ToTensorV2</span><span class="p">()])</span>  <span class="c1">#just convert to tensor
</span>
<span class="k">class</span> <span class="nc">TestDataSet</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="c1"># initialize imagepath, transforms:
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">image_paths</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transforms</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">)</span>

    <span class="c1"># define main function to read image, apply transform function and return the transformed images.
</span>    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">image_path</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># albumentations
</span>            <span class="n">transformed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transforms</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">transformed</span><span class="p">[</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">image</span> <span class="c1"># return tensors of equal dtype and size
</span>        <span class="c1"># image is size 3x1024x1024 and mask and bin_mask is size 1x1024x1024 (need dummy dimension to match dimension)
</span>
<span class="c1"># define dataloading function to use above dataset to return train and val dataloaders:
</span><span class="k">def</span> <span class="nf">load_test_dataset</span><span class="p">():</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">TestDataSet</span><span class="p">(</span><span class="n">image_paths</span><span class="o">=</span><span class="n">test_images</span><span class="p">,</span> <span class="n">transforms</span><span class="o">=</span><span class="n">test_transforms</span><span class="p">)</span>
    <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_dataloader</span>  <span class="c1"># return train and val dataloaders
</span></code></pre></div></div> <p>Then write a function to return the predicted segmentation tissue map (for all classes) for the image to generate HiResCAM for:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image_path</span> <span class="o">=</span> <span class="c1"># your own path to folder containing images to test for CAM
</span><span class="n">test_images</span> <span class="o">=</span> <span class="c1"># complete path of the single image to test for CAM within image_path
</span><span class="n">test_dataloader</span> <span class="o">=</span> <span class="nf">load_test_dataset</span><span class="p">()</span> 
<span class="nd">@torch.no_grad</span><span class="p">()</span>  <span class="c1">#decorator to disable gradient calc
</span><span class="k">def</span> <span class="nf">return_image_mask</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">weight_dir</span> <span class="o">=</span> <span class="c1"># your own path to the saved model weights
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">weight_dir</span><span class="p">))</span>  <span class="c1">#load model weights
</span>    <span class="n">pbar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">'</span><span class="s">Inference</span><span class="sh">'</span><span class="p">,</span> <span class="n">colour</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">image_path</span><span class="p">)</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>  <span class="c1"># eval stage
</span>        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>  <span class="c1">#move tensor to gpu
</span>        <span class="n">prediction</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">()</span> <span class="c1">#softmax for multiclass
</span>    <span class="k">return</span> <span class="n">prediction</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="nf">return_image_mask</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">test_dataloader</span><span class="p">,</span><span class="n">device</span><span class="p">)</span> <span class="c1">#predicted segmentation tissue map
</span></code></pre></div></div> <p>Then, preprocess the image (imagenet mean/std) to generate HiResCAM for:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rgb_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">))</span>
<span class="n">rgb_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                <span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span> <span class="c1">#imagenet mean/std
</span></code></pre></div></div> <p>Then, define a class to return the sum of the predictions of a specific chosen class (ex. class ECM for skin tissue), or the “target”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SemanticSegmentationTarget</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">category</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">category</span> <span class="o">=</span> <span class="n">category</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">mask</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model_output</span><span class="p">):</span>
        <span class="nf">return </span><span class="p">(</span><span class="n">model_output</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">category</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:</span> <span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">mask</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span>
</code></pre></div></div> <p>Then, below is the most important part: For a specific chosen class (ex. class ECM for skin tissue), return the “target”, choose a layer of interest from the loaded model, choose a CAM method (we choose HiResCAM, which was explained above) and utilize pytorch-grad-cam’s functions to generate our CAM image!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">he_mask</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:].</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
<span class="c1"># for skin: {"corneum" : 1,"spinosum": 2,"hairshaft":3,"hairfollicle":4,"smoothmuscle":5,"oil":6,"sweat":7,"nerve":8,"bloodvessel":9,"ecm":10,"fat":11,"white/background":12}
</span><span class="n">class_category</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># 6 = oil glands
</span><span class="n">he_mask_float</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">float32</span><span class="p">(</span><span class="n">he_mask</span> <span class="o">==</span> <span class="n">class_category</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="nc">SemanticSegmentationTarget</span><span class="p">(</span><span class="n">class_category</span><span class="p">,</span> <span class="n">he_mask_float</span><span class="p">)]</span> <span class="c1"># return targets
</span><span class="n">target_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">layer4</span><span class="p">]</span> <span class="c1"># we choose last layer
</span><span class="k">with</span> <span class="nc">HiResCAM</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> <span class="n">target_layers</span> <span class="o">=</span> <span class="n">target_layers</span><span class="p">,</span> <span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span> <span class="k">as</span> <span class="n">cam</span><span class="p">:</span>
    <span class="n">grayscale_cam</span> <span class="o">=</span> <span class="nf">cam</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">,</span><span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)[</span><span class="mi">0</span><span class="p">,:]</span> <span class="c1"># return CAM
</span>    <span class="n">cam_image</span> <span class="o">=</span> <span class="nf">show_cam_on_image</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">,</span> <span class="n">grayscale_cam</span><span class="p">,</span> <span class="n">use_rgb</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># overlay CAM with original rgb image
</span><span class="n">Image</span><span class="p">.</span><span class="nf">fromarray</span><span class="p">(</span><span class="n">cam_image</span><span class="p">)</span> <span class="c1">#visualize resulting CAM
</span></code></pre></div></div> <p>Finally, let’s look at some examples of CAMs for some of my skin H&amp;E tissue images, and hopefully we’ll see that my trained model is actually focusing on the right parts of the images and not cheating!</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/white_background_GradCAMElementWise-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/white_background_GradCAMElementWise-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/white_background_GradCAMElementWise-1400.webp"></source> <img src="/assets/images/CAMs/white_background_GradCAMElementWise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="White/background GradCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/white_background_HiResCAM-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/white_background_HiResCAM-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/white_background_HiResCAM-1400.webp"></source> <img src="/assets/images/CAMs/white_background_HiResCAM.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="White/background HiResCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/bloodvessel_GradCAMElementWise-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/bloodvessel_GradCAMElementWise-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/bloodvessel_GradCAMElementWise-1400.webp"></source> <img src="/assets/images/CAMs/bloodvessel_GradCAMElementWise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Blood vessel GradCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/bloodvessel_HiResCAM-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/bloodvessel_HiResCAM-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/bloodvessel_HiResCAM-1400.webp"></source> <img src="/assets/images/CAMs/bloodvessel_HiResCAM.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Blood vessel HiResCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/oil_GradCAMElementWise-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/oil_GradCAMElementWise-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/oil_GradCAMElementWise-1400.webp"></source> <img src="/assets/images/CAMs/oil_GradCAMElementWise.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Oil GradCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/images/CAMs/oil_HiResCAM-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/images/CAMs/oil_HiResCAM-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/images/CAMs/oil_HiResCAM-1400.webp"></source> <img src="/assets/images/CAMs/oil_HiResCAM.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Oil HiResCAM" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p>The left images are GradCAMs, and the right images are HiResCAMs Now let’s analyze the images of each row. The first row are the CAMs for the background, and we can see that there isn’t a big difference between the two, except that HiResCAM does show a more “accurate” depiction, as it is a faithful explanation after all. The second row are the CAMs for the blood vessels, and we can see that HiResCAM also has a more “accurate” depiction while GradCAM shows activation scores for spots that are not blood vessel-specific. Lastly, the third row are the CAMs for the oil glands, and this is where GradCAM is a bit misleading. GradCAM does highlight the oil glands successfully, but when looking at HiResCAM, we can see that the model doesn’t only look at oil glands. Therefore, with HiResCAM, I can see that the model is also mostly looking at nearby ECM and hair follicle areas for segmenting oil glands, which is quite interesting.</p> <p>The last example like above is the reason why we must continue to explore and try different types of CAMs, and also explore other options of explainable AI as well. Hope this helps!</p> <hr> <p><em>Image credits to:</em></p> <ul> <li><a href="http://cnnlocalization.csail.mit.edu/" rel="external nofollow noopener" target="_blank">Image Classification CAM Diagram</a></li> <li><a href="https://arxiv.org/pdf/2011.08891.pdf" rel="external nofollow noopener" target="_blank">HiResCAM Diagram</a></li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Kevin (Won June) Cho. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>