---
layout: post
title:  Full Review of Latent/Stable Diffusion Part 1!
date:   2023-02-04
description: 
tags: deep-learning machine-learning latent-diffusion stable-diffusion generative-models
categories: posts
---
<blockquote>
    Welcome to my first blog post! From now on, I'll be trying to do weekly updates on any interesting, recent AI-related topic.
</blockquote>

For this week, I'll do an in-depth review on the famous, famous latent/stable diffusion model which basically started the stable diffusion boom last year. 
It really is the "hot topic" right now, as the generative models are taking over the AI industry! For reference purposes, the stable diffusion paper that started it all is named "High-Resolution Image Synthesis with Latent Diffusion Models", and can be found [here](https://arxiv.org/pdf/2112.10752.pdf).
<strong> Let's dive right in!


<img src = "/assets/images/welcome-pikachu.png" width = "400" height = "400" class = "center">
<figcaption>Cute pikachu drawing made from DALLE-2!</figcaption>

---

# *Table of Contents:* 
## [Background](#background) (Part 1)
- ###  [Introduction](#introduction)
- ### [Stable Diffusion vs GAN](#stable-diffusion-vs-gan)

## [Stable Diffusion](#stable-diffusion) (Part 2)
- ### [Model Architecture](#model-architecture)
- ### [Experiments & Results](#experiment-results)
- ### [Variation of Stable Diffusion](#variation-stable-diffusion)


---
<a id="background"></a>
##  **Background:**
<a id="introduction"></a>
###  **Introduction:** 
Latent diffusion models (LDM), or henceforth mentioned as stable diffusion models (SD), are a type of a diffusion model 
developed for the purpose of image synthesis by Rombach et al. of last year. In Machine Learning, a "model" is either a generative or discriminative. 
The diagram below clearly shows the difference between the two:
<br>
<img src = "/assets/images/generative_v_discriminative.png" width = "523" height = "293" class = "center">
<figcaption>Diagram showing difference between generative and discriminative models.</figcaption>
<br>
As shown above, the discriminative model tries to tell the difference of a writing of 0 and 1 by just simply drawing a decision boundary
through the data space. Therefore, a discriminative model just has to model the posterior $$ p(y|x) $$ for label y and data sample x. 
It doesn't need to model the probability distribution of the entire data space to do this. For example, if you are familiar with SVMs, which is a type
of a discriminative classifier/model, we know that the model's objective is to find the support vectors that maximize the distance between the support vectors 
and the decision boundary, which is called a margin. Most of the time, the support vectors are very few data points that lie near the decision boundary (hyperplane)â€“ 
the majority of the other data samples simply don't matter. 

On the other hand, a generative model aims to model the probability distribution $$ p(x,y) $$ of the entire data space.
for cases where there is no label (semi-supervised/unsupervised cases), a generative model aims to model $$ p(x) $$ instead. We can see in the diagram above that the generative model
has come up with a probability distribution of the 0's and the 1's that well-represents the true, hidden probability distribution of the entire data space. Therefore, the generative 
model generally has to "work harder" to achieve its goals. However, this unique nature of generative models allows it to "generate" synthesized data by using sampling methods from the 
modeled probability distribution, just like the cute pikachu drawing I generated by using DALLE-2, which is a type of generative model.
Likewise, a stable diffusion model is a type of generative model. Generative models can be broadly classified into four types: 
<ul>
    <li>1. <strong>Flow-based models:</strong> Flow-based generative models are quite unique in that they utilize a method called "normalizing flow". Normalizing flow uses the change of variable theorem, which allows us to estimate a more complex probability density function for our model. 
This is hugely beneficial but also comes at a cost- during backpropagation, the derivative would be impossible or too hard to calculate. Therefore, we will later see that this is why stable diffusion utilizes the gaussian distribution in its noising process, even though it is much simpler than the "real world" distribution.
Normalizing flow is essentially a complex distribution modeled by a chain of invertible transformation functions. The probability density function is tractable, meaning the learning process is simply based on minimizing the negative log-likelihood over the given dataset. A critical drawback, however, is that normalizing models have limitations in that 
the transformations must all be invertible (for change of variable theorem to work) and determinants must be efficiently calculated (for backpropagation). More on flow-based models can be mentioned later in the blog.</li>
    <li>2. <strong>Autoregressive models:</strong> Autoregressive generative models, like their name suggests, means performing regression on its self. General autoregression means predicting a future outcome based on the 
previous data of that outcome. The general idea is that autoregressive models model the joint probability space $$p(x)$$ by utilizing the chain rule $$p(x,y) = p(y|x)p(x)$$, meaning that it is ultimately a product of conditional distributions. Like normalizing flows, defining the complex product of
conditional distribution is no easy task, and autoregressive models do this by utilizing the deep neural networks. In this case, outputs of the neural network is fed back as input, with the layers being one or more convolutional layers. Like normalizing flow models, the probability distribution is tractable, but 
the sampling process is slower as it is sequential by nature (sequential conditionals). More on autoregressive models can be mentioned in later blogs as well.</li>
    <li>3. <strong>Generative Adversarial Networks (GAN): GANs will be covered in more detail in the section below. </strong>
    <li>4. <strong>Latent variable models:</strong> <strong>Stable diffusion models</strong> belong to this type. This will be covered in more detail 
in the section below.
---

<a id="stable-diffusion-vs-gan"></a>
## **Stable Diffusion vs GAN:**
Surprisingly, diffusion models are not new at all! Stable diffusion is a type of diffusion model, and like its name suggests,
is actually based on the diffusion from thermodynamics! The forward diffusion process is where random (usually Gaussian) noise 
is introduced to an image until the image is pure noise, and the reverse diffusion process is where the model is trained so that 
the model is able to generate data samples from the noise that is representative of the true data distribution. Before diving deeper
into stable diffusion, let's first compare stable diffusion and GAN, because before stable diffusion's emergence as the SOTA (state-of-the-art)
generative model, GAN and its variants have been the SOTA generative model.





</ul>
<hr>

All diagrams/information/papers utilized from online are listed below:
<br>
1. https://developers.google.com/machine-learning/gan/generative
2. xxx

