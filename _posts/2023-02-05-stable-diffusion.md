---
layout: post
title:  Full Review of Latent/Stable Diffusion
date:   2023-02-04
description: 
tags: deep-learning machine-learning latent-diffusion stable-diffusion generative-models
categories: posts
---
<blockquote>
    Welcome to my first blog post! From now on, I'll be trying to do weekly updates on any interesting, recent AI-related topic.
</blockquote>

For this week, I'll do an in-depth review on the famous, famous latent/stable diffusion model which basically started the stable diffusion boom last year. 
The reason I'm doing this is because I'm trying to develop a stable diffusion model for my research purposes, which is virtual H&E staining of unstained tissue microarrays. 
Maybe I could do another blog post on this if the project progresses, or about previous works on this topic. For reference purposes, the stable diffusion paper named "High-Resolution Image Synthesis with Latent Diffusion Models" can be found [here](https://arxiv.org/pdf/2112.10752.pdf).
<strong> Let's dive right in!


<img src = "/assets/images/welcome-pikachu.png" width = "400" height = "400" class = "center">
<figcaption>Cute pikachu drawing made from DALLE-2!</figcaption>

# *Table of Contents:* 
## [Background](#background)
- ###  [Introduction](#introduction)
- ### [Stable Diffusion vs GAN](#stable-diffusion-vs-gan)

## [Stable Diffusion](#stable-diffusion)
- ### [Model Architecture](#model-architecture)
- ### [Experiments & Results](#experiment-results)

## [Variation of Stable Diffusion](#variation-stable-diffusion)
- ### [xx](#xx)

---
<a id="background"></a>
## Background:
<a id="introduction"></a>
### Introduction:
Latent diffusion models (LDM), or henceforth mentioned as stable diffusion models (SD), are a type of a diffusion model 
developed for the purpose of image synthesis by Rombach et al. of last year. In Machine Learning, a "model" is either a generative or discriminative. 
The diagram below clearly shows the difference between the two:
<br>
<img src = "/assets/images/generative_v_discriminative.png" width = "523" height = "293" class = "center">
<figcaption>Diagram showing difference between generative and discriminative models.</figcaption>
<br>
As shown above, the discriminative model tries to tell the difference of a writing of 0 and 1 by just simply drawing a decision boundary
through the data space. Therefore, a discriminative model just has to model the posterior $$ p(y|x) $$ for label y and data sample x. 
It doesn't need to model the probability distribution of the entire data space to do this. For example, if you are familiar with SVMs, which is a type
of a discriminative classifier/model, we know that the model's objective is to find the support vectors that maximize the distance between the support vectors 
and the decision boundary, which is called a margin. Most of the time, the support vectors are very few data points that lie near the decision boundary (hyperplane)– 
the majority of the other data samples simply don't matter. 

On the other hand, a generative model aims to model the probability distribution $$ p(x,y) $$ of the entire data space.
for cases where there is no label (semi-supervised/unsupervised cases), a generative model aims to model $$ p(x) $$ instead. We can see in the diagram above that the generative model
has come up with a probability distribution of the 0's and the 1's that well-represents the true, hidden probability distribution of the entire data space. Therefore, the generative 
model generally has to "work harder" to achieve its goals. However, this unique nature of generative models allows it to "generate" synthesized data by using sampling methods from the 
modeled probability distribution, just like the cute pikachu drawing I generated by using DALLE-2, which is a type of generative model called transformers. 
Likewise, a stable diffusion model is a type of generative model. Generative models can be broadly classified into four types: 
<ul>
    <li>1. <strong>Flow-based models:</strong> Flow-based models, </li>
    <li>2. <strong>Autoregressive models:</strong> Autoregressive models, like their name suggests, means performing regression on its self. General autoregression means predicting a future outcome based on the 
previous data of that outcome. However, in deep-learning space, deep autoregression is utilized– in this case, outputs of the neural network is fed back as input, with the layers being one or more convolutional
layers. The general idea is that autoregressive models model the joint probability space $$ p(x) $$ by utilizing the chain rule $$ p(x,y) = p(y|x)p(x) $$ More on autoregressive models can be mentioned in later blogs as well.</li>
    <li>3. <strong>Generative Adversarial Networks (GAN): All type of GANS belong to this type– this will be covered in more detail in the sections below. </strong>
    <li>4. <strong>Latent variable models:</strong> <strong>Stable diffusion models</strong> and variational autoencoders belong to this type. Like GANs, this will be covered in more detail
in the sections below. d

---
<a id="stable-diffusion-vs-gan"></a>
### Stable Diffusion vs GAN:
Surprisingly, diffusion models are not new at all! Stable diffusion is a type of diffusion model, a diffusion model is b
</ul>
<hr>

All diagrams/information/papers utilized from online are listed below:
<br>
1. https://developers.google.com/machine-learning/gan/generative
2. xxx

