---
layout: post
title:  How to use Class Activation Maps (CAM) for Explainable AI in Semantic Segmentation!
date:   2023-10-20
description: Introduction to CAM, explanation of GradCAM and HiResCAM in semantic segmentation, showcasing some example code for CAM and CAM image results in my project!
tags: deep-learning machine-learning image-segmentation project-update
categories: posts
---

In this post, I will briefly describe Class Activation Maps (CAM) and some of its popular subtypes, usages in semantic segmentation,
and then finally post some code and results in utilizing CAM in my own semantic segmentation project using [H&E images](/projects/1_project/).

---

## **Table of Contents:**
- ### [What are Class Activation Maps (CAM)?](#what-are-cam)
- ### [CAMs in Image Semantic Segmentation:](#cams-in-image-seg)
- ### [Utilizing CAM in My Project](#cam-in-my-proj)

---

<a id="what-are-cam"></a>
## **What are Class Activation Maps (CAM) ?**
*Class activation maps*, henceforth referred to as *CAM*, can be thought of as heatmaps that can highlight the regions of a query image that are most important for the
network's classification or segmentation decision. Before introducing CAMs, *why do we want to look at CAMs in the first place?*

CAMs are probably one of the most popular and easy-to-do explainable AI metrics, which aim to understand and interpret the decisions made by built machine learning models. 
Let's come up with an example where CAMs would be useful: Let's say I work for a AI company and we built a machine learning model that looks at a chest x-ray and is able to detect a rare disease that occurs 1% of the time with 99% sensitivity (so the model itself is
sensitive (or has high recall), so it isn't saying no all the time and has 99% accuracy). However, by utilizing CAMs before deploying the model, I find out that the model is actually only looking at a circle at the corner of the x-ray image to detect
the rare disease. Upon checking the images used for training/validation/testing, they all had a circle at the corner that the hospital used to specify that this was a positive image. **Without CAMs, we would have no idea this was the case
and would've been deploying a completely useless model!**

Like the above made-up example, traditionally, CAMs were developed to be used for image classification, as utilization of CAM was limited to specific types of architectures,
which are CNNs with global average pooling (GAP) and a final fully connected layer. Most image classification models use GAP and a final fully connected layer followed by the output
activation function (assume multi-class, so we use softmax) for turning logits into final prediction probability/results. The last layer before the GAP and fully connected layer is the layer that holds the "feature map",
which captures subtle, fine semantic details of the training images. 
The implementation of CAM is beautifully summarized in the diagram below:

<img src = "/assets/images/CAMs/CAM_summarized.jpg" width = "835" height = "392" class = "center">
<figcaption> Diagram showing CAM for an image classification task. </figcaption>
<br>

As seen in the above diagram, the class activation map is generated by a linear combination of all the $$n$$ weights (weights in the fully connected layer) for the specific class (in the above case, the Australian terrier)
and all the $$n$$ feature maps. For example, in the diagram above, we can tell that $$w_2$$ would have a higher weight than $$w_1$$ since the feature map $$F_2$$ has to do with the Australian terrier. Mathematically, the CAM can be represented as:
<p>
$$Y_c = \sum_{k} {w_k}^{c} \cdot \frac{1}{Z} \sum_{i}\sum_{j} A_{(i,j)}^{k}$$
$$Y_c = \sum_{k} {w_k}^{c} \cdot F^{k} \quad (1)$$
</p>
,where $$Y_c$$ is the activation score (CAM) for $$c$$, which is the specific class (like Australian terrier). $$k$$ is the number of feature maps and therefore $${w_k}^{c}$$ is the weight for feature map $$k$$ for class $$c$$. 
Lastly, $$A_{(i,j)}^{k}$$ is the feature map $$k$$ for pixel coordinate $$(i,j)$$, which is summed over all $$i$$ and $$j$$ and divided by total number of pixels $$Z$$ to return our global average pooled feature map $$F^{k}$$. 
We can see that $$\frac{1}{Z} \sum_{i}\sum_{j}$$ is the mathematical representation of global average pooling (GAP). Finally, since feature maps are downsampled compared to the original image size,
we need to perform bi-linear interpolation on the CAM to upsample for us to visualize the overlay of the CAM on the query image like shown in the diagram above.

However, as mentioned above, the general CAM method requires an architecture that includes all of the three:
1. A feature map, or the penultimate layer of the model.
2. A global average pooling operation to the feature map.
3. A final fully connected layer followed by an activation function to produce prediction labels. 

Since not all methods satisfy all three requirements above, CAM was only limited to certain types of architecture, making it unavailable to use for other types of CNN architectures that are dense like VGG, handle multi-modal inputs/perform reinforcement learning.

<a id="cams-in-image-seg"></a>
## **CAMs in Image Semantic Segmentation:**

As mentioned previously, the three requirements for CAM severely limited the usage to certain types of architectures, and was totally not applicable for non-CNN based models such as vision transformers, or ViTs (to be fair, when CAM was first
released, transformers were not a thing yet). Therefore, GradCAM, or Gradient-weighted CAMs, were widely used which essentially replaces the weights of the fully connected layer with calculated gradients that flow back into the last
convolutional layer. To calculate gradients consider taking the gradient of the activation score for class $$c$$ ($$Y^{c}$$) with respect to feature map $$F^{k}$$ from the above CAM equation, or equation #1:
<p>
$$ Y_c = \sum_{k} {w_k}^{c} \cdot F^{k}$$
$$\frac{\delta Y^{c}}{\delta F^{k}} = \frac{\frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}}} {\frac{\delta F^{k}}{\delta A_{(i,j)}^{k}}} $$
$$\text{Recall from above: } F^{k} = \frac{1}{Z} \sum_{i}\sum_{j} \text{ so: } \frac{\delta F^{k}}{\delta A_{(i,j)}^{k}} = \frac{1}{Z}$$
$$\text{Then: } \frac{\delta Y^{c}}{\delta F^{k}} = \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \cdot Z $$
$$\text{Recall from above: } Y_c = \sum_{k} {w_k}^{c} \cdot F^{k} \text{ so: } \frac{\delta Y^{c}}{\delta F^{k}} = {w_k}^{c}$$
$$\text{Then: } {w_k}^{c} = Z \cdot \frac{\delta Y^{c}}{\delta A_{(i,j)}^{k}} \quad (2)$$
</p>



Now, even though CAMs were widely used for classification, they could definitely also be used for semantic segmentation tasks where each pixel is labeled as a class. In this case,

HiResCAM:

<a id="cam-in-my-proj"></a>
## **Utilizing CAM in My Project:**

Now that we know the differences between GradCAM and HiResCAM, below is the code that I utilized to generate CAM for my skin H&E tissue images to 
assess how my DeepLabv3+ image segmentation segments the classes and if it is cheating or not!

First import relevant packages, including our pytorch-grad-cam library from the official [repo](https://github.com/jacobgil/pytorch-grad-cam):

```python
import torch
import torch.functional as F
import numpy as np
import requests
import torchvision
from PIL import Image
from torch.utils.data import Dataset, DataLoader
import cv2
import os
from natsort import natsorted
from tqdm import tqdm
from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image
from pytorch_grad_cam import GradCAM, HiResCAM, GradCAMElementWise, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, EigenGradCAM, FullGrad, LayerCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
Image.MAX_IMAGE_PIXELS = None
import segmentation_models_pytorch as smp
import albumentations as A
from albumentations.pytorch import ToTensorV2
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```
Then, write your own function to load relevant model (for me, my DeepLabV3+ segmentation model) and if necessary, your own dataloading function as well:
```python
test_transforms = A.Compose([ToTensorV2()])  #just convert to tensor

class TestDataSet(Dataset):
    # initialize imagepath, transforms:
    def __init__(self, image_paths: list, transforms=None):
        self.image_paths = image_paths
        self.transforms = transforms

    def __len__(self):
        return len(self.image_paths)

    # define main function to read image, apply transform function and return the transformed images.
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)
        image = np.array(image)
        if self.transforms is not None:  # albumentations
            transformed = self.transforms(image=image)
            image = transformed['image']
        return image # return tensors of equal dtype and size
        # image is size 3x1024x1024 and mask and bin_mask is size 1x1024x1024 (need dummy dimension to match dimension)

# define dataloading function to use above dataset to return train and val dataloaders:
def load_test_dataset():
    test_dataset = TestDataSet(image_paths=test_images, transforms=test_transforms)
    test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, num_workers=0, pin_memory=True, shuffle=False)
    return test_dataloader  # return train and val dataloaders
```
Then write a function to return the predicted segmentation tissue map (for all classes) for the image to generate HiResCAM for:
```python
image_path = # your own path to folder containing images to test for CAM
test_images = # complete path of the single image to test for CAM within image_path
test_dataloader = load_test_dataset() 
@torch.no_grad()  #decorator to disable gradient calc
def return_image_mask(model, dataloader, device):
    weight_dir = # your own path to the saved model weights
    model.load_state_dict(torch.load(weight_dir))  #load model weights
    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Inference', colour='red')
    for idx, (images, image_path) in pbar:
        model.eval()  # eval stage
        images = images.to(device, dtype=torch.float)  #move tensor to gpu
        prediction = model(images)
        prediction = torch.nn.functional.softmax(prediction, dim=1).cpu() #softmax for multiclass
    return prediction
prediction = return_image_mask(model,test_dataloader,device) #predicted segmentation tissue map
```
Then, preprocess the image (imagenet mean/std) to generate HiResCAM for:
```python
rgb_img = np.array(Image.open(image_path))
rgb_img = np.float32(rgb_img) / 255
input_tensor = preprocess_image(rgb_img, mean=[0.485, 0.456, 0.406],
                                std=[0.229, 0.224, 0.225]) #imagenet mean/std
```
Then, define a class to return the sum of the predictions of a specific chosen class (ex. class ECM for skin tissue), or the "target". 

```python
class SemanticSegmentationTarget:
    def __init__(self, category, mask):
        self.category = category
        self.mask = torch.from_numpy(mask)
        if torch.cuda.is_available():
            self.mask = self.mask.cuda()

    def __call__(self, model_output):
        return (model_output[self.category, :, : ] * self.mask).sum()
```
Then, below is the most important part: For a specific chosen class (ex. class ECM for skin tissue), return the "target", choose a layer of interest from the loaded model,
choose a CAM method (we choose HiResCAM, which was explained above) and utilize pytorch-grad-cam's functions to generate our CAM image!
```python
he_mask = prediction[0, :, :, :].argmax(axis=0).detach().cpu().numpy()
# for skin: {"corneum" : 1,"spinosum": 2,"hairshaft":3,"hairfollicle":4,"smoothmuscle":5,"oil":6,"sweat":7,"nerve":8,"bloodvessel":9,"ecm":10,"fat":11,"white/background":12}
class_category = 6 # 6 = oil glands
he_mask_float = np.float32(he_mask == class_category)
targets = [SemanticSegmentationTarget(class_category, he_mask_float)] # return targets
target_layers = [model.encoder.layer4] # we choose last layer
with HiResCAM(model = model, target_layers = target_layers, use_cuda = torch.cuda.is_available()) as cam:
    grayscale_cam = cam(input_tensor=input_tensor,targets=targets)[0,:] # return CAM
    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True) # overlay CAM with original rgb image
Image.fromarray(cam_image) #visualize resulting CAM
```

Finally, let's look at some examples of CAMs for some of my skin H&E tissue images, and hopefully we'll see that
my trained model is actually focusing on the right parts of the images and not cheating!

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/images/CAMs/white_background_GradCAMElementWise.png" title="White/background GradCAM" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/images/CAMs/white_background_HiResCAM.png" title="White/background HiResCAM" class="img-fluid rounded z-depth-1" %}
    </div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/images/CAMs/bloodvessel_GradCAMElementWise.png" title="Blood vessel GradCAM" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/images/CAMs/bloodvessel_HiResCAM.png" title="Blood vessel HiResCAM" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/images/CAMs/oil_GradCAMElementWise.png" title="Oil GradCAM" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/images/CAMs/oil_HiResCAM.png" title="Oil HiResCAM" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

The left images are HiResCAMs, and the right images are GradCAMs. Now let's analyze the images of each row. The first row are the CAMs for the background, and 
we can see that there isn't a big difference between the two, except that HiResCAM does show a more "accurate" depiction, as it is a faithful explanation after all. 




---

*Image credits to:*
- [Image Classification CAM Diagram](http://cnnlocalization.csail.mit.edu/)
