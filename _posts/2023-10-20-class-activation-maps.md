---
layout: post
title:  How to use Class Activation Maps (CAM) for Explainable AI in Semantic Segmentation!
date:   2023-10-20
description: Introduction to CAM, explanation of GradCAM and HiResCAM in semantic segmentation, showcasing some example code for CAM and CAM image results in my project!
tags: deep-learning machine-learning image-segmentation project-update
categories: posts
---

In this post, I will briefly describe Class Activation Maps (CAM) and some of its popular subtypes, usages in semantic segmentation,
and then finally post some code and results in utilizing CAM in my own semantic segmentation project using [H&E images](/projects/1_project/).

---

## **Table of Contents:**
- ### [What are Class Activation Maps (CAM)?](#what-are-cam)
- ### [CAMs in Image Semantic Segmentation:](#cams-in-image-seg)
- ### [Utilizing CAM in My Project](#cam-in-my-proj)

---

<a id="what-are-cam"></a>
## **What are Class Activation Maps (CAM) ?**
*Class activation maps*, henceforth referred to as *CAM*, can be thought of as heatmaps that can highlight the regions of a query image that are most important for the
network's classification or segmentation decision. Before introducing CAMs, *why do we want to look at CAMs in the first place?*

CAMs are probably one of the most popular and easy-to-do explainable AI metrics, which aim to understand and interpret the decisions made by built machine learning models. 
Let's come up with an example where CAMs would be useful: Let's say I work for a AI company and we built a machine learning model that looks at a chest x-ray and is able to detect a rare disease that occurs 1% of the time with 99% sensitivity (so the model itself is
sensitive (or has high recall), so it isn't saying no all the time and has 99% accuracy). However, by utilizing CAMs before deploying the model, I find out that the model is actually only looking at a circle at the corner of the x-ray image to detect
the rare disease. Upon checking the images used for training/validation/testing, they all had a circle at the corner that the hospital used to specify that this was a positive image. **Without CAMs, we would have no idea this was the case
and would've been deploying a completely useless model!**

Like the above made-up example, traditionally, CAMs were developed to be used for image classification, as utilization of CAM was limited to specific types of architectures,
which are CNNs with global average pooling (GAP) and a final fully connected layer. Most image classification models use GAP and a final fully connected layer followed by the output
activation function (assume multi-class, so we use softmax) for turning logits into final prediction probability/results. The last layer before the GAP and fully connected layer is the layer that holds the "feature map",
which captures subtle, fine semantic details of the training images. 
The implementation of CAM is beautifully summarized in the diagram below:

<img src = "/assets/images/CAMs/CAM_summarized.jpg" width = "835" height = "392" class = "center">
<figcaption> Diagram showing CAM for an image classification task. </figcaption>
<br>

As seen in the above diagram, the class activation map is generated by a linear combination of all the $$n$$ weights (weights in the fully connected layer) for the specific class (in the above case, the Australian terrier)
and all the $$n$$ feature maps. For example, in the diagram above, we can tell that $$w_2$$ would have a higher weight than $$w_1$$ since the feature map $$F_2$$ has to do with the Australian terrier. Mathematically, the CAM can be represented as:
<p>
$$CAM_c(x,y) = \sum_{k} {w_k}^{c} \cdot f_k(x,y) $$
</p>
,where $$k$$ is the number of feature maps and $$CAM_c(x,y)$$ is the activation of pixel coordinate $$(x,y)$$ for $$c$$, which is the specific class (like Australian terrier). Finally, since feature maps are downsampled compared to the original image size,
we need to perform bi-linear interpolation on the CAM to upsample for us to visualize the overlay of the CAM on the query image like shown in the diagram above.

However, as mentioned above, the general CAM method requires an architecture that includes all of the three:
1. A feature map, or the penultimate layer of the model.
2. A global average pooling operation to the feature map.
3. A final fully connected layer followed by an activation function to produce prediction labels. 

Since not all methods satisfy all three requirements above, CAM was only limited to certain types of architecture, making it unavailable to use for other types of architectures that are dense like VGG, handle multi-modal inputs/perform reinforcement learning.

<a id="cams-in-image-seg"></a>
## **CAMs in Image Semantic Segmentation:**

Now, even though CAMs were widely used for classification, they could definitely also be used for semantic segmentation tasks where each pixel is labeled as a class. In this case,
multiple heatmaps will be generated for 

As mentioned previously, the three requirements for CAM severely limited the usage to certain types of architectures, and was totally not applicable for non-CNN based models such as vision transformers, or ViTs (to be fair, when CAM was first
released, transformers were not a thing yet). 
LayerCAM:

HiResCAM:

<a id="cam-in-my-proj"></a>
## **Utilizing CAM in My Project:**


Code:


```python
CAMmethod = "GradCAMElementWise"
with GradCAMElementWise(model = model, target_layers = target_layers, use_cuda = torch.cuda.is_available()) as cam:
    grayscale_cam = cam(input_tensor=input_tensor,targets=targets)[0,:]
    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)

```




---

*Image credits to:*
- [Image Classification CAM Diagram](http://cnnlocalization.csail.mit.edu/)

